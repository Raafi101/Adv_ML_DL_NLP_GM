{"cells":[{"cell_type":"markdown","id":"meaningful-integral","metadata":{"id":"meaningful-integral"},"source":["## AML7_NLP: Practical illustration of typical NLP steps"]},{"cell_type":"code","execution_count":null,"id":"traditional-trace","metadata":{"id":"traditional-trace"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","id":"known-uganda","metadata":{"id":"known-uganda"},"source":["### Extract text from HTML"]},{"cell_type":"markdown","id":"confirmed-madonna","metadata":{"id":"confirmed-madonna"},"source":["The course repo has a subdirectory called `html` which includes some example HTML files. "]},{"cell_type":"code","execution_count":null,"id":"unavailable-prize","metadata":{"id":"unavailable-prize","outputId":"23e906f3-2866-45ac-9214-7a44d9d94144"},"outputs":[{"data":{"text/plain":["['article1.html', 'article2.html', 'article3.html', 'article4.html']"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["%sx ls html/"]},{"cell_type":"markdown","id":"focused-waste","metadata":{"id":"focused-waste"},"source":["Select one of those files to use as an example, and take a look at its HTML content."]},{"cell_type":"code","execution_count":null,"id":"perfect-eclipse","metadata":{"id":"perfect-eclipse","outputId":"6f6f2f77-f160-42b3-c4db-72584eb85563"},"outputs":[{"name":"stdout","output_type":"stream","text":["['<!DOCTYPE html>\\n', '<html lang=\"en\">\\n', '<head>\\n', \" <title>The current state of machine intelligence 3.0 - O'Reilly Media</title>\\n\", '</head>\\n', '<body>\\n', '<div id=\"article-body\">\\n', '<p>Almost a year ago, we published our now-annual <a href=\"https://www.oreilly.com/ideas/the-current-state-of-machine-intelligence-2-0\">landscape</a> of machine intelligence companies, and goodness have we seen a lot of activity since then. This year\\'s landscape has <em>a third more companies</em> than our first one did two years ago, and it feels even more futile to try to be comprehensive, since this just scratches the surface of all of the activity out there.</p>\\n', '\\n', '<p>As has been the case for the last couple of years, our fund still obsesses over \"problem first\" machine intelligence -- we\\'ve invested in 35 machine intelligence companies solving 35 meaningful problems in areas from security to recruiting to software development. (Our fund focuses on the future of work, so there are some machine intelligence domains where we invest more than others.)</p>\\n', '\\n', '<p>At the same time, the hype around machine intelligence methods continues to grow: the words \"deep learning\" now equally represent a series of meaningful breakthroughs (wonderful) but also a hyped phrase like \"big data\" (not so good!). We care about whether a founder uses the right method to solve a problem, not the fanciest one. We favor those who apply technology thoughtfully.</p>\\n', '\\n', '<p>What\\'s the biggest change in the last year? We are getting inbound inquiries from a different mix of people. For <a href=\"https://medium.com/@shivon/the-current-state-of-machine-intelligence-f76c20db2fe1#.ei67436b7\">v1.0</a>, we heard almost exclusively from founders and academics. Then came a healthy mix of investors, both private and public. Now overwhelmingly we have heard from existing companies trying to figure out how to transform their businesses using machine intelligence.</p>\\n', '</div>\\n', '\\n', '<div class=\"text-group\">\\n', ' <p class=\"dek\">Shivon Zilis is a partner and founding member of Bloomberg Beta, an early-stage VC firm that invests in startups making work better, with a focus on machine intelligence. She\\'s particularly fascinated by intelligence tools and industry applications. Like any good Canadian, she spends her spare time playing hockey and snowboarding. She holds a degree in economics and philosophy from Yale.</p>\\n', '    \\n', ' <p class=\"dek\">James Cham is a Partner at Bloomberg Beta based in Palo Alto. He invests in data-centric and machine learning-related companies. He was a principal at Trinity Ventures and Vice President at Bessemer Venture Partners, where he worked with investments like Dropcam, Twilio, and LifeLock. He\\'s a former software developer and management consultant. He has an M.B.A. from the Massachusetts Institute of Technology and was an undergraduate in computer science at Harvard College.</p>\\n', '\\n', ' <p class=\"copyright\">&copy; 2016 O\\'Reilly Media, Inc. All trademarks and registered trademarks appearing on oreilly.com are the property of their respective owners.</p><br>\\n', '</div>\\n', '\\n', '</body>\\n', '</html>\\n']\n"]}],"source":["file = \"html/article1.html\"\n","print(open(file, \"r\").readlines())"]},{"cell_type":"markdown","id":"capable-federation","metadata":{"id":"capable-federation"},"source":["Next, use [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to extract text out of the HTML. Following the [DOM](https://en.wikipedia.org/wiki/Document_Object_Model) structure of the HTML document, select the `<div/>` that encloses the article text, then iterate through the `<p/>` paragraphs to extract the text from each."]},{"cell_type":"code","execution_count":null,"id":"outside-monroe","metadata":{"id":"outside-monroe"},"outputs":[],"source":["# !conda install -c conda-forge beautifulsoup4 --yes"]},{"cell_type":"code","execution_count":null,"id":"written-illinois","metadata":{"id":"written-illinois"},"outputs":[],"source":["# %sx read -p 'y/n?: '"]},{"cell_type":"code","execution_count":null,"id":"civil-latino","metadata":{"id":"civil-latino","outputId":"c67e9b03-a7e1-47b8-9894-8cbfe06cc7a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Almost a year ago, we published our now-annual landscape of machine intelligence companies, and goodness have we seen a lot of activity since then. This year's landscape has a third more companies than our first one did two years ago, and it feels even more futile to try to be comprehensive, since this just scratches the surface of all of the activity out there.\n","As has been the case for the last couple of years, our fund still obsesses over \"problem first\" machine intelligence -- we've invested in 35 machine intelligence companies solving 35 meaningful problems in areas from security to recruiting to software development. (Our fund focuses on the future of work, so there are some machine intelligence domains where we invest more than others.)\n","At the same time, the hype around machine intelligence methods continues to grow: the words \"deep learning\" now equally represent a series of meaningful breakthroughs (wonderful) but also a hyped phrase like \"big data\" (not so good!). We care about whether a founder uses the right method to solve a problem, not the fanciest one. We favor those who apply technology thoughtfully.\n","What's the biggest change in the last year? We are getting inbound inquiries from a different mix of people. For v1.0, we heard almost exclusively from founders and academics. Then came a healthy mix of investors, both private and public. Now overwhelmingly we have heard from existing companies trying to figure out how to transform their businesses using machine intelligence.\n"]}],"source":["from bs4 import BeautifulSoup\n","\n","with open(file) as f:\n","    soup = BeautifulSoup(f, \"html.parser\")\n","\n","    for div in soup.find_all(\"div\", id=\"article-body\"):\n","        for p in div.find_all(\"p\"):\n","            print(p.get_text())"]},{"cell_type":"markdown","id":"affecting-aluminum","metadata":{"editable":true,"id":"affecting-aluminum"},"source":["### Concerns about characters"]},{"cell_type":"markdown","id":"cardiovascular-drama","metadata":{"editable":true,"id":"cardiovascular-drama"},"source":["The following shows examples of how to use [codecs](https://docs.python.org/3/library/codecs.html) and [normalize unicode](https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize). NB: the example text comes from the article \"[Metal umlat](https://en.wikipedia.org/wiki/Metal_umlaut)\"."]},{"cell_type":"code","execution_count":null,"id":"alien-pavilion","metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"id":"alien-pavilion","outputId":"dd0d71d9-aa53-490f-e846-db4b8745588c"},"outputs":[{"data":{"text/plain":["str"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["x = \"Rinôçérôse screams ﬂow not unlike an encyclopædia, \\\n","'TECHNICIÄNS ÖF SPÅCE SHIP EÅRTH THIS IS YÖÜR CÄPTÅIN SPEÄKING YÖÜR ØÅPTÅIN IS DEA̋D' to Spın̈al Tap.\"\n","type(x)"]},{"cell_type":"markdown","id":"consistent-dispute","metadata":{"editable":true,"id":"consistent-dispute"},"source":["The variable `x` is a *string* in Python:"]},{"cell_type":"code","execution_count":null,"id":"registered-breast","metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"id":"registered-breast","outputId":"3d5f8506-c92f-49f2-c9d5-90877fb7cdd4"},"outputs":[{"data":{"text/plain":["'\"Rinôçérôse screams ﬂow not unlike an encyclopædia, \\'TECHNICIÄNS ÖF SPÅCE SHIP EÅRTH THIS IS YÖÜR CÄPTÅIN SPEÄKING YÖÜR ØÅPTÅIN IS DEA̋D\\' to Spın̈al Tap.\"'"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["repr(x)"]},{"cell_type":"markdown","id":"pressed-system","metadata":{"editable":true,"id":"pressed-system"},"source":["Its translation into [ASCII](http://www.asciitable.com/) is unusable by parsers:"]},{"cell_type":"code","execution_count":null,"id":"latest-comedy","metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"id":"latest-comedy","outputId":"8df8e12c-7da4-4f10-b227-946fdbed8342"},"outputs":[{"data":{"text/plain":["'\"Rin\\\\xf4\\\\xe7\\\\xe9r\\\\xf4se screams \\\\ufb02ow not unlike an encyclop\\\\xe6dia, \\'TECHNICI\\\\xc4NS \\\\xd6F SP\\\\xc5CE SHIP E\\\\xc5RTH THIS IS Y\\\\xd6\\\\xdcR C\\\\xc4PT\\\\xc5IN SPE\\\\xc4KING Y\\\\xd6\\\\xdcR \\\\xd8\\\\xc5PT\\\\xc5IN IS DEA\\\\u030bD\\' to Sp\\\\u0131n\\\\u0308al Tap.\"'"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["ascii(x)"]},{"cell_type":"markdown","id":"characteristic-welcome","metadata":{"editable":true,"id":"characteristic-welcome"},"source":["Encoding as [UTF-8](http://unicode.org/faq/utf_bom.html) doesn't help much:\n","\n","https://docs.python.org/3.1/whatsnew/3.0.html#text-vs-data-instead-of-unicode-vs-8-bit"]},{"cell_type":"code","execution_count":null,"id":"iraqi-binding","metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"id":"iraqi-binding","outputId":"d84af8c8-956f-49c8-c465-8c63f8d167eb"},"outputs":[{"data":{"text/plain":["b\"Rin\\xc3\\xb4\\xc3\\xa7\\xc3\\xa9r\\xc3\\xb4se screams \\xef\\xac\\x82ow not unlike an encyclop\\xc3\\xa6dia, 'TECHNICI\\xc3\\x84NS \\xc3\\x96F SP\\xc3\\x85CE SHIP E\\xc3\\x85RTH THIS IS Y\\xc3\\x96\\xc3\\x9cR C\\xc3\\x84PT\\xc3\\x85IN SPE\\xc3\\x84KING Y\\xc3\\x96\\xc3\\x9cR \\xc3\\x98\\xc3\\x85PT\\xc3\\x85IN IS DEA\\xcc\\x8bD' to Sp\\xc4\\xb1n\\xcc\\x88al Tap.\""]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["x.encode('utf8')  "]},{"cell_type":"markdown","id":"concrete-framing","metadata":{"editable":true,"id":"concrete-framing"},"source":["Ignoring difficult characters is perhaps an even worse strategy:"]},{"cell_type":"code","execution_count":null,"id":"athletic-paragraph","metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"id":"athletic-paragraph","outputId":"18b00af0-c0ce-4e16-e658-910b6c76b068"},"outputs":[{"data":{"text/plain":["b\"Rinrse screams ow not unlike an encyclopdia, 'TECHNICINS F SPCE SHIP ERTH THIS IS YR CPTIN SPEKING YR PTIN IS DEAD' to Spnal Tap.\""]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["x.encode('ascii', 'ignore')"]},{"cell_type":"markdown","id":"pretty-commodity","metadata":{"editable":true,"id":"pretty-commodity"},"source":["However, one can *normalize* then encode…"]},{"cell_type":"code","execution_count":null,"id":"public-walter","metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"id":"public-walter","outputId":"89c2c732-24fc-4a7d-c3b9-531ade019cf3"},"outputs":[{"data":{"text/plain":["b\"Rinocerose screams flow not unlike an encyclopdia, 'TECHNICIANS OF SPACE SHIP EARTH THIS IS YOUR CAPTAIN SPEAKING YOUR APTAIN IS DEAD' to Spnal Tap.\""]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["import unicodedata\n","\n","unicodedata.normalize('NFKD', x).encode('ascii','ignore')"]},{"cell_type":"markdown","id":"outdoor-roller","metadata":{"editable":true,"id":"outdoor-roller"},"source":["Even before this normalization and encoding, you may need to convert some characters explicitly **before** parsing. For example:"]},{"cell_type":"code","execution_count":null,"id":"spectacular-capacity","metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"id":"spectacular-capacity","outputId":"71bd09b3-06b7-4597-ece3-b542424f9155"},"outputs":[{"data":{"text/plain":["\"'The sky \\\\u201cabove\\\\u201d the port \\\\u2026 was the color of \\\\u2018cable television\\\\u2019 \\\\u2013 tuned to the Weather Channel\\\\xae'\""]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["x = \"The sky “above” the port … was the color of ‘cable television’ – tuned to the Weather Channel®\"\n","ascii(x)"]},{"cell_type":"markdown","id":"breeding-assumption","metadata":{"editable":true,"id":"breeding-assumption"},"source":["Consider the results for that line:"]},{"cell_type":"code","execution_count":null,"id":"fifth-japan","metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"id":"fifth-japan","outputId":"43014e26-01ca-44de-c5a8-c619eadc1f95"},"outputs":[{"data":{"text/plain":["b'The sky above the port ... was the color of cable television  tuned to the Weather Channel'"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["unicodedata.normalize('NFKD', x).encode('ascii', 'ignore')"]},{"cell_type":"markdown","id":"champion-thermal","metadata":{"editable":true,"id":"champion-thermal"},"source":["...which still drops characters that may be important for parsing a sentence.\n","\n","So an alternative would be:"]},{"cell_type":"code","execution_count":null,"id":"american-debut","metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"id":"american-debut","outputId":"bf96971e-57b3-4d2e-a27a-7e248feaf4f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["The sky \"above\" the port ... was the color of 'cable television' - tuned to the Weather Channel\n"]}],"source":["x = x.replace('“', '\"').replace('”', '\"')\n","x = x.replace(\"‘\", \"'\").replace(\"’\", \"'\")\n","x = x.replace('…', '...').replace('–', '-')\n","x = unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8')\n","print(x)"]},{"cell_type":"code","execution_count":null,"id":"comparable-monitoring","metadata":{"id":"comparable-monitoring"},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"geological-ottawa","metadata":{"id":"geological-ottawa"},"source":["### Statistical parsing\n","\n","NLP used to be mostly concerned about transformational grammars, linguistic theory by Chomsky, etc. \n","\n","ML techniques allow much simpler approaches called statistical parsing."]},{"cell_type":"markdown","id":"private-relations","metadata":{"id":"private-relations"},"source":["<font color='blue'>Probabilistic methods</font> split texts into sentences, annotate words with part-of-speech, chunk noun phrases, resolve named entities, estimate sentiment scores, etc."]},{"cell_type":"code","execution_count":null,"id":"alternate-seafood","metadata":{"id":"alternate-seafood"},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"considered-scout","metadata":{"editable":true,"id":"considered-scout"},"source":["Let's start with a simple paragraph:"]},{"cell_type":"code","execution_count":null,"id":"indian-family","metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"id":"indian-family","outputId":"7be37348-f993-4f3f-be42-6daf2cf3bb0b"},"outputs":[{"data":{"text/plain":["\"'\\\\nIncreasingly, customers send text to interact or leave comments, \\\\nwhich provides a wealth of data for text mining.  That’s a great \\\\nstarting point for developing custom search, content recommenders, \\\\nand even AI applications.\\\\n'\""]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["text = \"\"\"\n","Increasingly, customers send text to interact or leave comments, \n","which provides a wealth of data for text mining.  That’s a great \n","starting point for developing custom search, content recommenders, \n","and even AI applications.\n","\"\"\"\n","repr(text)"]},{"cell_type":"markdown","id":"danish-shooting","metadata":{"editable":true,"id":"danish-shooting"},"source":["Notice how there are explicit *line breaks* in the text. Let's write some code to flow the paragraph without any line breaks:"]},{"cell_type":"code","execution_count":null,"id":"dominant-europe","metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"id":"dominant-europe","outputId":"1e8e2ac5-5af2-49cb-9e86-536ee09ea0d1"},"outputs":[{"data":{"text/plain":["\"'Increasingly, customers send text to interact or leave comments, which provides a wealth of data for text mining.  That’s a great starting point for developing custom search, content recommenders, and even AI applications.'\""]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["text = \" \".join(map(lambda x: x.strip(), text.split(\"\\n\"))).strip()\n","repr(text)"]},{"cell_type":"code","execution_count":null,"id":"selective-oriental","metadata":{"id":"selective-oriental"},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"earlier-tunnel","metadata":{"id":"earlier-tunnel"},"source":["#### We’ll use a few popular NLP resources for parsing text:\n","\n","* [spaCy](https://spacy.io/)  – one of the top NLP libraries in Python \n","\n","* [TextBlob](http://textblob.readthedocs.io/) – a Python library that provides a consistent API for leveraging other resources\n","\n","* [WordNet](https://wordnet.princeton.edu/) – think of it as somewhere between a large thesaurus and a database\n","\n","One important step is to annotate the words in each sentence with a tag that describes its part of speech: noun, verb, adjective, determinant, adverb, etc.\n"]},{"cell_type":"code","execution_count":null,"id":"emerging-messenger","metadata":{"id":"emerging-messenger"},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"czech-television","metadata":{"editable":true,"id":"czech-television"},"source":["#### Splitting sentences"]},{"cell_type":"markdown","id":"beginning-bachelor","metadata":{"editable":true,"id":"beginning-bachelor"},"source":["Now we can use **spaCy** to **split** the paragraph into sentences:"]},{"cell_type":"code","execution_count":null,"id":"thick-kenya","metadata":{"id":"thick-kenya"},"outputs":[],"source":["# !conda install -c conda-forge spacy --yes\n","\n","# pip install -U spacy\n","# python -m spacy download en_core_web_sm"]},{"cell_type":"code","execution_count":null,"id":"bronze-palestinian","metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"id":"bronze-palestinian"},"outputs":[],"source":["import spacy\n","\n","# Load English tokenizer, tagger, parser and NER\n","nlp = spacy.load(\"en_core_web_sm\") "]},{"cell_type":"code","execution_count":null,"id":"provincial-batch","metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"id":"provincial-batch","outputId":"f545619f-d959-47ef-c58b-ad105acfa19b"},"outputs":[{"name":"stdout","output_type":"stream","text":[">  Increasingly, customers send text to interact or leave comments, which provides a wealth of data for text mining.  \n",">  That’s a great starting point for developing custom search, content recommenders, and even AI applications.\n"]}],"source":["doc = nlp(text) #, parse=True)\n","\n","for span in doc.sents:\n","    print(\"> \", span)"]},{"cell_type":"code","execution_count":null,"id":"sophisticated-mother","metadata":{"id":"sophisticated-mother"},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"available-underground","metadata":{"id":"available-underground"},"source":["#### PoS annotation"]},{"cell_type":"markdown","id":"multiple-disabled","metadata":{"editable":true,"id":"multiple-disabled"},"source":["Next we take a sentence and **annotate** it with part-of-speech (PoS) tags:"]},{"cell_type":"code","execution_count":null,"id":"moderate-listing","metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"id":"moderate-listing","outputId":"9f0f3128-c65b-415f-c750-1dfdcb58c030"},"outputs":[{"name":"stdout","output_type":"stream","text":["0 Increasingly RB ADV\n","1 , , PUNCT\n","2 customers NNS NOUN\n","3 send VBP VERB\n","4 text NN NOUN\n","5 to IN ADP\n","6 interact VB VERB\n","7 or CC CCONJ\n","8 leave VB VERB\n","9 comments NNS NOUN\n","10 , , PUNCT\n","11 which WDT DET\n","12 provides VBZ VERB\n","13 a DT DET\n","14 wealth NN NOUN\n","15 of IN ADP\n","16 data NNS NOUN\n","17 for IN ADP\n","18 text NN NOUN\n","19 mining NN NOUN\n","20 . . PUNCT\n","21   _SP SPACE\n","22 That DT DET\n","23 ’s VBZ VERB\n","24 a DT DET\n","25 great JJ ADJ\n","26 starting NN NOUN\n","27 point NN NOUN\n","28 for IN ADP\n","29 developing VBG VERB\n","30 custom NN NOUN\n","31 search NN NOUN\n","32 , , PUNCT\n","33 content NN NOUN\n","34 recommenders NNS NOUN\n","35 , , PUNCT\n","36 and CC CCONJ\n","37 even RB ADV\n","38 AI NNP PROPN\n","39 applications NNS NOUN\n","40 . . PUNCT\n"]}],"source":["for span in doc.sents:\n","    for i in range(span.start, span.end):\n","        token = doc[i]\n","        print(i, token.text, token.tag_, token.pos_)"]},{"cell_type":"markdown","id":"colored-cemetery","metadata":{"editable":true,"id":"colored-cemetery"},"source":["Given these annotations for part-of-speech tags, we can <font color='blue'>**lemmatize**</font> nouns and verbs to get their root forms. This will also singularize the plural nouns:"]},{"cell_type":"code","execution_count":null,"id":"expired-bones","metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"id":"expired-bones","outputId":"3e1a547e-4beb-4b00-b62c-6f033ca41b28"},"outputs":[{"name":"stdout","output_type":"stream","text":["0 Increasingly RB ADV increasingly\n","1 , , PUNCT ,\n","2 customers NNS NOUN customer\n","3 send VBP VERB send\n","4 text NN NOUN text\n","5 to IN ADP to\n","6 interact VB VERB interact\n","7 or CC CCONJ or\n","8 leave VB VERB leave\n","9 comments NNS NOUN comment\n","10 , , PUNCT ,\n","11 which WDT DET which\n","12 provides VBZ VERB provide\n","13 a DT DET a\n","14 wealth NN NOUN wealth\n","15 of IN ADP of\n","16 data NNS NOUN datum\n","17 for IN ADP for\n","18 text NN NOUN text\n","19 mining NN NOUN mining\n","20 . . PUNCT .\n","21   _SP SPACE  \n","22 That DT DET that\n","23 ’s VBZ VERB ’\n","24 a DT DET a\n","25 great JJ ADJ great\n","26 starting NN NOUN starting\n","27 point NN NOUN point\n","28 for IN ADP for\n","29 developing VBG VERB develop\n","30 custom NN NOUN custom\n","31 search NN NOUN search\n","32 , , PUNCT ,\n","33 content NN NOUN content\n","34 recommenders NNS NOUN recommender\n","35 , , PUNCT ,\n","36 and CC CCONJ and\n","37 even RB ADV even\n","38 AI NNP PROPN AI\n","39 applications NNS NOUN application\n","40 . . PUNCT .\n"]}],"source":["for span in doc.sents:\n","    for i in range(span.start, span.end):\n","        token = doc[i]\n","        print(i, token.text, token.tag_, token.pos_, token.lemma_)"]},{"cell_type":"code","execution_count":null,"id":"popular-giving","metadata":{"id":"popular-giving"},"outputs":[],"source":["### Lemmatization - for other languages \n","\n","# https://towardsdatascience.com/state-of-the-art-multilingual-lemmatization-f303e8ff1a8"]},{"cell_type":"markdown","id":"sized-triumph","metadata":{"editable":true,"id":"sized-triumph"},"source":["We can also lookup synonyms and definitions for each word, using **synsets** from [WordNet](https://wordnet.princeton.edu/).\n","\n","Have in mind that `spaCy` is designed to be an **opinionated** API, and it omits support for much of the value of `WordNet`. However, we can use [TextBlob](http://textblob.readthedocs.io/) instead:"]},{"cell_type":"code","execution_count":null,"id":"needed-massachusetts","metadata":{"id":"needed-massachusetts"},"outputs":[],"source":["# ! conda install -c conda-forge textblob --yes"]},{"cell_type":"code","execution_count":null,"id":"baking-norway","metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"id":"baking-norway","outputId":"988cf1b0-f0ef-437a-e2c6-5657d7db3c82"},"outputs":[{"name":"stdout","output_type":"stream","text":["Synset('remark.n.01') a statement that expresses a personal opinion or belief or adds information\n","Synset('comment.n.02') a written explanation or criticism or illustration that is added to a book or other textual material\n","Synset('gossip.n.02') a report (often malicious) about the behavior of other people\n","Synset('comment.v.01') make or write a comment on\n","Synset('comment.v.02') explain or interpret something\n","Synset('gloss.v.02') provide interlinear explanations for words or phrases\n"]}],"source":["from textblob import Word\n","\n","w = Word(\"comments\")\n","\n","for synset, definition in zip(w.get_synsets(), w.define()):\n","    print(synset, definition)"]},{"cell_type":"code","execution_count":null,"id":"wireless-modern","metadata":{"id":"wireless-modern"},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"wicked-pasta","metadata":{"editable":true,"id":"wicked-pasta"},"source":["### Noun phrase chunking"]},{"cell_type":"markdown","id":"above-stations","metadata":{"editable":true,"id":"above-stations"},"source":["Sometimes it's useful to use **noun phrase chunking** to extract key phrases…"]},{"cell_type":"code","execution_count":null,"id":"widespread-carry","metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"id":"widespread-carry","outputId":"3203fa65-748e-4c7d-e67e-38342cb05b7b"},"outputs":[{"data":{"text/plain":["\"That's a great starting point for developing custom search, content recommenders, and even AI applications.\""]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["text = \"That's a great starting point for developing custom search, content recommenders, and even AI applications.\"\n","doc = nlp(text)\n","\n","repr(doc)"]},{"cell_type":"markdown","id":"liable-dream","metadata":{"editable":true,"id":"liable-dream"},"source":["First let's look at the individual **keywords**:"]},{"cell_type":"code","execution_count":null,"id":"continental-uncertainty","metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"id":"continental-uncertainty","outputId":"cfc59d71-5132-4bd4-d5dc-877732564cc5"},"outputs":[{"name":"stdout","output_type":"stream","text":["That\n","'s\n","a\n","great\n","starting\n","point\n","for\n","developing\n","custom\n","search\n",",\n","content\n","recommenders\n",",\n","and\n","even\n","AI\n","applications\n",".\n"]}],"source":["for token in doc:\n","    print(token)"]},{"cell_type":"markdown","id":"voluntary-while","metadata":{"editable":true,"id":"voluntary-while"},"source":["Contrast those results with **noun phrases**:"]},{"cell_type":"code","execution_count":null,"id":"strong-tribune","metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"id":"strong-tribune","outputId":"0e5b6a6e-5021-4995-bc6f-4329f7ff4ed3"},"outputs":[{"name":"stdout","output_type":"stream","text":["a great starting point\n","custom search\n","content recommenders\n","even AI applications\n"]}],"source":["for np in doc.noun_chunks:\n","    print(np)"]},{"cell_type":"markdown","id":"threatened-grenada","metadata":{"editable":true,"id":"threatened-grenada"},"source":["There's definitely more information in the key phrase `custom search` than there is in the individual keywords `custom` and `search`."]},{"cell_type":"code","execution_count":null,"id":"brazilian-career","metadata":{"id":"brazilian-career","outputId":"3c24d8fb-43cb-4239-a1d1-1bf663fc77b1"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('That', 'DT'), (\"'s\", 'VBZ'), ('a', 'DT'), ('great', 'JJ'), ('starting', 'JJ'), ('point', 'NN'), ('for', 'IN'), ('developing', 'VBG'), ('custom', 'JJ'), ('search', 'NN'), ('content', 'NN'), ('recommenders', 'NNS'), ('and', 'CC'), ('even', 'RB'), ('AI', 'NNP'), ('applications', 'NNS')]\n"]}],"source":["from textblob import TextBlob\n","\n","blob = TextBlob(text)\n","print(blob.tags)"]},{"cell_type":"code","execution_count":null,"id":"warming-paint","metadata":{"id":"warming-paint","outputId":"5e0050ef-5df3-40e0-d1c3-c7bd48486d8c"},"outputs":[{"name":"stdout","output_type":"stream","text":["['custom search', 'content recommenders', 'ai']\n"]}],"source":["print(blob.noun_phrases)"]},{"cell_type":"code","execution_count":null,"id":"addressed-heating","metadata":{"id":"addressed-heating"},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"copyrighted-search","metadata":{"editable":true,"id":"copyrighted-search"},"source":["### Named entity resolution (NER)"]},{"cell_type":"markdown","id":"democratic-bracelet","metadata":{"editable":true,"id":"democratic-bracelet"},"source":["Often you want to identify the proper noun phrases within a text. For that we can use *named-entity resolution* [NER](https://spacy.io/docs/usage/entity-recognition):"]},{"cell_type":"code","execution_count":null,"id":"worldwide-comparative","metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"id":"worldwide-comparative","outputId":"2d26d748-4bfa-4ac3-beca-f37749420f54"},"outputs":[{"data":{"text/plain":["\"He'd been trained by the best, McCoy Pauley and Bobby Quine, legends in Memphis, and now Chiba City as well.\""]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["# import spacy\n","# nlp = spacy.load(\"en\")\n","\n","text = \"He'd been trained by the best, McCoy Pauley and Bobby Quine, legends in Memphis, and now Chiba City as well.\"\n","doc = nlp(text)\n","\n","repr(doc)"]},{"cell_type":"markdown","id":"thirty-parcel","metadata":{"editable":true,"id":"thirty-parcel"},"source":["Clearly these entities enrich the key phrases extracted from a text:"]},{"cell_type":"code","execution_count":null,"id":"historic-basement","metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"id":"historic-basement","outputId":"3e494fe8-9fa6-47eb-8bad-a3ab6b86fe07"},"outputs":[{"name":"stdout","output_type":"stream","text":["PERSON McCoy Pauley\n","PERSON Bobby Quine\n","GPE Memphis\n","GPE Chiba City\n"]}],"source":["for entity in doc.ents:\n","    print(entity.label_, entity.text)"]},{"cell_type":"code","execution_count":null,"id":"reflected-reference","metadata":{"id":"reflected-reference"},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"induced-lotus","metadata":{"editable":true,"id":"induced-lotus"},"source":["### Store annotated text as JSON files"]},{"cell_type":"markdown","id":"beneficial-manchester","metadata":{"editable":true,"id":"beneficial-manchester"},"source":["Much of the preceding NLP code has been worked into a small library, and we'll call functions from that library to help keep these notebooks more readable. Take a look at the source code in `pynlp.py`, and an example usage:"]},{"cell_type":"code","execution_count":null,"id":"brazilian-machine","metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"id":"brazilian-machine"},"outputs":[],"source":["import pynlp\n","\n","html_file = \"html/article1.html\"\n","json_file = \"a1.json\"\n","\n","pynlp.full_parse(html_file, json_file)"]},{"cell_type":"markdown","id":"graduate-oracle","metadata":{"editable":true,"id":"graduate-oracle"},"source":["That extracts text from HTML in the first article, then stores the parsed and annotated text as JSON, one line per sentence. Let's look at the first two sentences:"]},{"cell_type":"code","execution_count":null,"id":"closing-magnitude","metadata":{"collapsed":true,"editable":true,"jupyter":{"outputs_hidden":true},"id":"closing-magnitude","outputId":"eedbe0b7-6c90-4dd6-99ae-91c3e17dcf50"},"outputs":[{"data":{"text/plain":["['[[\"Almost\", \"almost\", \"RB\"], [\"a\", \"a\", \"DT\"], [\"year\", \"year\", \"NN\"], [\"ago\", \"ago\", \"RB\"], [\",\", \",\", \".\"], [\"we\", \"we\", \"PRP\"], [\"published\", \"publish\", \"VBD\"], [\"our\", \"our\", \"PRP$\"], [\"now\", \"now\", \"RB\"], [\"-\", \"-\", \".\"], [\"annual\", \"annual\", \"JJ\"], [\"landscape\", \"landscape\", \"NN\"], [\"of\", \"of\", \"IN\"], [\"machine\", \"machine\", \"NN\"], [\"intelligence\", \"intelligence\", \"NN\"], [\"companies\", \"company\", \"NNS\"], [\",\", \",\", \".\"], [\"and\", \"and\", \"CC\"], [\"goodness\", \"goodness\", \"NN\"], [\"have\", \"have\", \"VBP\"], [\"we\", \"we\", \"PRP\"], [\"seen\", \"see\", \"VBN\"], [\"a\", \"a\", \"DT\"], [\"lot\", \"lot\", \"NN\"], [\"of\", \"of\", \"IN\"], [\"activity\", \"activity\", \"NN\"], [\"since\", \"since\", \"IN\"], [\"then\", \"then\", \"RB\"], [\".\", \".\", \".\"]]',\n"," '[[\"This\", \"this\", \"DT\"], [\"year\", \"year\", \"NN\"], [\"\\'s\", \"\\'s\", \"POS\"], [\"landscape\", \"landscape\", \"NN\"], [\"has\", \"has\", \"VBZ\"], [\"a\", \"a\", \"DT\"], [\"third\", \"third\", \"RB\"], [\"more\", \"more\", \"JJR\"], [\"companies\", \"company\", \"NNS\"], [\"than\", \"than\", \"IN\"], [\"our\", \"our\", \"PRP$\"], [\"first\", \"first\", \"JJ\"], [\"one\", \"one\", \"NN\"], [\"did\", \"did\", \"VBD\"], [\"two\", \"two\", \"CD\"], [\"years\", \"year\", \"NNS\"], [\"ago\", \"ago\", \"RB\"], [\",\", \",\", \".\"], [\"and\", \"and\", \"CC\"], [\"it\", \"it\", \"PRP\"], [\"feels\", \"feel\", \"VBZ\"], [\"even\", \"even\", \"RB\"], [\"more\", \"more\", \"RBR\"], [\"futile\", \"futile\", \"JJ\"], [\"to\", \"to\", \"TO\"], [\"try\", \"try\", \"VB\"], [\"to\", \"to\", \"TO\"], [\"be\", \"be\", \"VB\"], [\"comprehensive\", \"comprehensive\", \"JJ\"], [\",\", \",\", \".\"], [\"since\", \"since\", \"IN\"], [\"this\", \"this\", \"DT\"], [\"just\", \"just\", \"RB\"], [\"scratches\", \"scratch\", \"VBZ\"], [\"the\", \"the\", \"DT\"], [\"surface\", \"surface\", \"NN\"], [\"of\", \"of\", \"IN\"], [\"all\", \"all\", \"DT\"], [\"of\", \"of\", \"IN\"], [\"the\", \"the\", \"DT\"], [\"activity\", \"activity\", \"NN\"], [\"out\", \"out\", \"RB\"], [\"there\", \"there\", \"RB\"], [\".\", \".\", \".\"]]',\n"," '[[\"As\", \"as\", \"IN\"], [\"has\", \"has\", \"VBZ\"], [\"been\", \"been\", \"VBN\"], [\"the\", \"the\", \"DT\"], [\"case\", \"case\", \"NN\"], [\"for\", \"for\", \"IN\"], [\"the\", \"the\", \"DT\"], [\"last\", \"last\", \"JJ\"], [\"couple\", \"couple\", \"NN\"], [\"of\", \"of\", \"IN\"], [\"years\", \"year\", \"NNS\"], [\",\", \",\", \".\"], [\"our\", \"our\", \"PRP$\"], [\"fund\", \"fund\", \"NN\"], [\"still\", \"still\", \"RB\"], [\"obsesses\", \"obsess\", \"VBZ\"], [\"over\", \"over\", \"IN\"], [\"\\\\\"\", \"\\\\\"\", \".\"], [\"problem\", \"problem\", \"VB\"], [\"first\", \"first\", \"RB\"], [\"\\\\\"\", \"\\\\\"\", \".\"], [\"machine\", \"machine\", \"NN\"], [\"intelligence\", \"intelligence\", \"NN\"], [\"--\", \"--\", \".\"], [\"we\", \"we\", \"PRP\"], [\"\\'ve\", \"\\'ve\", \"VB\"], [\"invested\", \"invest\", \"VBN\"], [\"in\", \"in\", \"IN\"], [\"35\", \"35\", \"CD\"], [\"machine\", \"machine\", \"NN\"], [\"intelligence\", \"intelligence\", \"NN\"], [\"companies\", \"company\", \"NNS\"], [\"solving\", \"solve\", \"VBG\"], [\"35\", \"35\", \"CD\"], [\"meaningful\", \"meaningful\", \"JJ\"], [\"problems\", \"problem\", \"NNS\"], [\"in\", \"in\", \"IN\"], [\"areas\", \"area\", \"NNS\"], [\"from\", \"from\", \"IN\"], [\"security\", \"security\", \"NN\"], [\"to\", \"to\", \"IN\"], [\"recruiting\", \"recruit\", \"VBG\"], [\"to\", \"to\", \"IN\"], [\"software\", \"software\", \"NN\"], [\"development\", \"development\", \"NN\"], [\".\", \".\", \".\"]]',\n"," '[[\"(\", \"(\", \".\"], [\"Our\", \"our\", \"PRP$\"], [\"fund\", \"fund\", \"NN\"], [\"focuses\", \"focus\", \"VBZ\"], [\"on\", \"on\", \"IN\"], [\"the\", \"the\", \"DT\"], [\"future\", \"future\", \"NN\"], [\"of\", \"of\", \"IN\"], [\"work\", \"work\", \"NN\"], [\",\", \",\", \".\"], [\"so\", \"so\", \"CC\"], [\"there\", \"there\", \"EX\"], [\"are\", \"are\", \"VBP\"], [\"some\", \"some\", \"DT\"], [\"machine\", \"machine\", \"NN\"], [\"intelligence\", \"intelligence\", \"NN\"], [\"domains\", \"domain\", \"NNS\"], [\"where\", \"where\", \"WRB\"], [\"we\", \"we\", \"PRP\"], [\"invest\", \"invest\", \"VBP\"], [\"more\", \"more\", \"JJR\"], [\"than\", \"than\", \"IN\"], [\"others\", \"other\", \"NNS\"], [\".\", \".\", \".\"], [\")\", \")\", \".\"]]',\n"," '[[\"At\", \"at\", \"IN\"], [\"the\", \"the\", \"DT\"], [\"same\", \"same\", \"JJ\"], [\"time\", \"time\", \"NN\"], [\",\", \",\", \".\"], [\"the\", \"the\", \"DT\"], [\"hype\", \"hype\", \"NN\"], [\"around\", \"around\", \"IN\"], [\"machine\", \"machine\", \"NN\"], [\"intelligence\", \"intelligence\", \"NN\"], [\"methods\", \"method\", \"NNS\"], [\"continues\", \"continue\", \"VBZ\"], [\"to\", \"to\", \"TO\"], [\"grow\", \"grow\", \"VB\"], [\":\", \":\", \".\"], [\"the\", \"the\", \"DT\"], [\"words\", \"word\", \"NNS\"], [\"\\\\\"\", \"\\\\\"\", \".\"], [\"deep\", \"deep\", \"JJ\"], [\"learning\", \"learning\", \"NN\"], [\"\\\\\"\", \"\\\\\"\", \".\"], [\"now\", \"now\", \"RB\"], [\"equally\", \"equally\", \"RB\"], [\"represent\", \"represent\", \"VBP\"], [\"a\", \"a\", \"DT\"], [\"series\", \"series\", \"NN\"], [\"of\", \"of\", \"IN\"], [\"meaningful\", \"meaningful\", \"JJ\"], [\"breakthroughs\", \"breakthrough\", \"NNS\"], [\"(\", \"(\", \".\"], [\"wonderful\", \"wonderful\", \"JJ\"], [\")\", \")\", \".\"], [\"but\", \"but\", \"CC\"], [\"also\", \"also\", \"RB\"], [\"a\", \"a\", \"DT\"], [\"hyped\", \"hype\", \"VBN\"], [\"phrase\", \"phrase\", \"NN\"], [\"like\", \"like\", \"IN\"], [\"\\\\\"\", \"\\\\\"\", \".\"], [\"big\", \"big\", \"JJ\"], [\"data\", \"datum\", \"NNS\"], [\"\\\\\"\", \"\\\\\"\", \".\"], [\"(\", \"(\", \".\"], [\"not\", \"not\", \"RB\"], [\"so\", \"so\", \"RB\"], [\"good\", \"good\", \"JJ\"], [\"!\", \"!\", \".\"], [\")\", \")\", \".\"], [\".\", \".\", \".\"]]',\n"," '[[\"We\", \"we\", \"PRP\"], [\"care\", \"care\", \"VBP\"], [\"about\", \"about\", \"IN\"], [\"whether\", \"whether\", \"IN\"], [\"a\", \"a\", \"DT\"], [\"founder\", \"founder\", \"NN\"], [\"uses\", \"use\", \"VBZ\"], [\"the\", \"the\", \"DT\"], [\"right\", \"right\", \"JJ\"], [\"method\", \"method\", \"NN\"], [\"to\", \"to\", \"TO\"], [\"solve\", \"solve\", \"VB\"], [\"a\", \"a\", \"DT\"], [\"problem\", \"problem\", \"NN\"], [\",\", \",\", \".\"], [\"not\", \"not\", \"RB\"], [\"the\", \"the\", \"DT\"], [\"fanciest\", \"fanciest\", \"JJ\"], [\"one\", \"one\", \"CD\"], [\".\", \".\", \".\"]]',\n"," '[[\"We\", \"we\", \"PRP\"], [\"favor\", \"favor\", \"VBP\"], [\"those\", \"those\", \"DT\"], [\"who\", \"who\", \"WP\"], [\"apply\", \"apply\", \"VBP\"], [\"technology\", \"technology\", \"NN\"], [\"thoughtfully\", \"thoughtfully\", \"RB\"], [\".\", \".\", \".\"]]',\n"," '[[\"What\", \"what\", \"WP\"], [\"\\'s\", \"\\'s\", \"VBZ\"], [\"the\", \"the\", \"DT\"], [\"biggest\", \"biggest\", \"JJS\"], [\"change\", \"change\", \"NN\"], [\"in\", \"in\", \"IN\"], [\"the\", \"the\", \"DT\"], [\"last\", \"last\", \"JJ\"], [\"year\", \"year\", \"NN\"], [\"?\", \"?\", \".\"]]',\n"," '[[\"We\", \"we\", \"PRP\"], [\"are\", \"are\", \"VBP\"], [\"getting\", \"get\", \"VBG\"], [\"inbound\", \"inbound\", \"JJ\"], [\"inquiries\", \"inquiry\", \"NNS\"], [\"from\", \"from\", \"IN\"], [\"a\", \"a\", \"DT\"], [\"different\", \"different\", \"JJ\"], [\"mix\", \"mix\", \"NN\"], [\"of\", \"of\", \"IN\"], [\"people\", \"people\", \"NNS\"], [\".\", \".\", \".\"]]',\n"," '[[\"For\", \"for\", \"IN\"], [\"v1.0\", \"v1.0\", \"NNP\"], [\",\", \",\", \".\"], [\"we\", \"we\", \"PRP\"], [\"heard\", \"hear\", \"VBD\"], [\"almost\", \"almost\", \"RB\"], [\"exclusively\", \"exclusively\", \"RB\"], [\"from\", \"from\", \"IN\"], [\"founders\", \"founder\", \"NNS\"], [\"and\", \"and\", \"CC\"], [\"academics\", \"academic\", \"NNS\"], [\".\", \".\", \".\"]]',\n"," '[[\"Then\", \"then\", \"RB\"], [\"came\", \"come\", \"VBD\"], [\"a\", \"a\", \"DT\"], [\"healthy\", \"healthy\", \"JJ\"], [\"mix\", \"mix\", \"NN\"], [\"of\", \"of\", \"IN\"], [\"investors\", \"investor\", \"NNS\"], [\",\", \",\", \".\"], [\"both\", \"both\", \"DT\"], [\"private\", \"private\", \"JJ\"], [\"and\", \"and\", \"CC\"], [\"public\", \"public\", \"JJ\"], [\".\", \".\", \".\"]]',\n"," '[[\"Now\", \"now\", \"RB\"], [\"overwhelmingly\", \"overwhelmingly\", \"RB\"], [\"we\", \"we\", \"PRP\"], [\"have\", \"have\", \"VBP\"], [\"heard\", \"hear\", \"VBN\"], [\"from\", \"from\", \"IN\"], [\"existing\", \"exist\", \"VBG\"], [\"companies\", \"company\", \"NNS\"], [\"trying\", \"try\", \"VBG\"], [\"to\", \"to\", \"TO\"], [\"figure\", \"figure\", \"VB\"], [\"out\", \"out\", \"RP\"], [\"how\", \"how\", \"WRB\"], [\"to\", \"to\", \"TO\"], [\"transform\", \"transform\", \"VB\"], [\"their\", \"their\", \"PRP$\"], [\"businesses\", \"business\", \"NNS\"], [\"using\", \"use\", \"VBG\"], [\"machine\", \"machine\", \"NN\"], [\"intelligence\", \"intelligence\", \"NN\"], [\".\", \".\", \".\"]]']"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["%sx more a1.json"]},{"cell_type":"markdown","id":"mighty-roulette","metadata":{"editable":true,"id":"mighty-roulette"},"source":["Extract/parse/save-to-JSON for each of the example HTML files:"]},{"cell_type":"code","execution_count":null,"id":"opposite-gardening","metadata":{"id":"opposite-gardening"},"outputs":[],"source":["html_file = \"html/article2.html\"\n","json_file = \"a2.json\"\n","\n","pynlp.full_parse(html_file, json_file)"]},{"cell_type":"code","execution_count":null,"id":"dated-occasions","metadata":{"id":"dated-occasions"},"outputs":[],"source":["html_file = \"html/article3.html\"\n","json_file = \"a3.json\"\n","\n","pynlp.full_parse(html_file, json_file)"]},{"cell_type":"code","execution_count":null,"id":"reflected-blackjack","metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"id":"reflected-blackjack"},"outputs":[],"source":["html_file = \"html/article4.html\"\n","json_file = \"a4.json\"\n","\n","pynlp.full_parse(html_file, json_file)"]},{"cell_type":"code","execution_count":null,"id":"curious-guest","metadata":{"collapsed":true,"editable":true,"jupyter":{"outputs_hidden":true},"id":"curious-guest","outputId":"fa081389-1fba-40a4-df20-d3950b86a411"},"outputs":[{"data":{"text/plain":["['[[\"Learning\", \"learning\", \"NN\"], [\"is\", \"is\", \"VBZ\"], [\"n\\'t\", \"n\\'t\", \"RB\"], [\"a\", \"a\", \"DT\"], [\"one\", \"one\", \"CD\"], [\"-\", \"-\", \".\"], [\"shot\", \"shot\", \"NN\"], [\"process\", \"process\", \"NN\"], [\":\", \":\", \".\"], [\"take\", \"take\", \"VB\"], [\"the\", \"the\", \"DT\"], [\"course\", \"course\", \"NN\"], [\",\", \",\", \".\"], [\"pass\", \"pass\", \"VB\"], [\"an\", \"an\", \"DT\"], [\"exam\", \"exam\", \"NN\"], [\",\", \",\", \".\"], [\"and\", \"and\", \"CC\"], [\"get\", \"get\", \"VB\"], [\"out\", \"out\", \"RP\"], [\".\", \".\", \".\"]]',\n"," '[[\"You\", \"you\", \"PRP\"], [\"learn\", \"learn\", \"VBP\"], [\"by\", \"by\", \"IN\"], [\"interacting\", \"interact\", \"VBG\"], [\"with\", \"with\", \"IN\"], [\"instructors\", \"instructor\", \"NNS\"], [\"and\", \"and\", \"CC\"], [\"students\", \"student\", \"NNS\"], [\",\", \",\", \".\"], [\"by\", \"by\", \"IN\"], [\"assessing\", \"assess\", \"VBG\"], [\"your\", \"your\", \"PRP$\"], [\"progress\", \"progress\", \"NN\"], [\",\", \",\", \".\"], [\"and\", \"and\", \"CC\"], [\"using\", \"use\", \"VBG\"], [\"that\", \"that\", \"DT\"], [\"to\", \"to\", \"TO\"], [\"plan\", \"plan\", \"VB\"], [\"your\", \"your\", \"PRP$\"], [\"next\", \"next\", \"JJ\"], [\"steps\", \"step\", \"NNS\"], [\".\", \".\", \".\"]]',\n"," '[[\"It\", \"it\", \"PRP\"], [\"\\'s\", \"\\'s\", \"VBZ\"], [\"an\", \"an\", \"DT\"], [\"ongoing\", \"ongoing\", \"JJ\"], [\"feedback\", \"feedback\", \"NN\"], [\"loop\", \"loop\", \"NN\"], [\"that\", \"that\", \"WDT\"], [\"involves\", \"involve\", \"VBZ\"], [\"everyone\", \"everyone\", \"NN\"], [\"in\", \"in\", \"IN\"], [\"the\", \"the\", \"DT\"], [\"classroom\", \"classroom\", \"NN\"], [\"(\", \"(\", \".\"], [\"whether\", \"whether\", \"IN\"], [\"the\", \"the\", \"DT\"], [\"classroom\", \"classroom\", \"NN\"], [\"is\", \"is\", \"VBZ\"], [\"virtual\", \"virtual\", \"JJ\"], [\"or\", \"or\", \"CC\"], [\"physical\", \"physical\", \"JJ\"], [\")\", \")\", \".\"], [\".\", \".\", \".\"]]',\n"," '[[\"O\\'Reilly\", \"o\\'reilly\", \"NNP\"], [\"Media\", \"media\", \"NNP\"], [\"has\", \"has\", \"VBZ\"], [\"always\", \"always\", \"RB\"], [\"been\", \"been\", \"VBN\"], [\"a\", \"a\", \"DT\"], [\"learning\", \"learn\", \"VBG\"], [\"company\", \"company\", \"NN\"], [\".\", \".\", \".\"]]',\n"," '[[\"When\", \"when\", \"WRB\"], [\"we\", \"we\", \"PRP\"], [\"began\", \"begin\", \"VBD\"], [\"to\", \"to\", \"TO\"], [\"publish\", \"publish\", \"VB\"], [\"books\", \"book\", \"NNS\"], [\"in\", \"in\", \"IN\"], [\"the\", \"the\", \"DT\"], [\"mid-\\'80s\", \"mid-\\'80s\", \"NNP\"], [\",\", \",\", \".\"], [\"our\", \"our\", \"PRP$\"], [\"editorial\", \"editorial\", \"JJ\"], [\"guidance\", \"guidance\", \"NN\"], [\"for\", \"for\", \"IN\"], [\"authors\", \"author\", \"NNS\"], [\"was\", \"was\", \"VBD\"], [\"to\", \"to\", \"TO\"], [\"write\", \"write\", \"VB\"], [\"as\", \"as\", \"IN\"], [\"if\", \"if\", \"IN\"], [\"they\", \"they\", \"PRP\"], [\"were\", \"were\", \"VBD\"], [\"a\", \"a\", \"DT\"], [\"friend\", \"friend\", \"NN\"], [\"looking\", \"look\", \"VBG\"], [\"over\", \"over\", \"IN\"], [\"the\", \"the\", \"DT\"], [\"readers\", \"reader\", \"NNS\"], [\"shoulder\", \"shoulder\", \"NN\"], [\",\", \",\", \".\"], [\"providing\", \"provide\", \"VBG\"], [\"wise\", \"wise\", \"JJ\"], [\"and\", \"and\", \"CC\"], [\"experienced\", \"experienced\", \"JJ\"], [\"advice\", \"advice\", \"NN\"], [\".\", \".\", \".\"]]',\n"," '[[\"In\", \"in\", \"IN\"], [\"2016\", \"2016\", \"CD\"], [\",\", \",\", \".\"], [\"we\", \"we\", \"PRP\"], [\"\\'re\", \"\\'re\", \"VBP\"], [\"about\", \"about\", \"RB\"], [\"much\", \"much\", \"RB\"], [\"more\", \"more\", \"JJR\"], [\"than\", \"than\", \"IN\"], [\"books\", \"book\", \"NNS\"], [\".\", \".\", \".\"]]',\n"," '[[\"We\", \"we\", \"PRP\"], [\"\\'ve\", \"\\'ve\", \"VB\"], [\"long\", \"long\", \"RB\"], [\"had\", \"have\", \"VBN\"], [\"instructional\", \"instructional\", \"JJ\"], [\"video\", \"video\", \"NN\"], [\"and\", \"and\", \"CC\"], [\"conferences\", \"conference\", \"NNS\"], [\".\", \".\", \".\"]]',\n"," '[[\"In\", \"in\", \"IN\"], [\"the\", \"the\", \"DT\"], [\"past\", \"past\", \"JJ\"], [\"year\", \"year\", \"NN\"], [\",\", \",\", \".\"], [\"we\", \"we\", \"PRP\"], [\"\\'ve\", \"\\'ve\", \"VB\"], [\"introduced\", \"introduce\", \"VBN\"], [\"live\", \"live\", \"JJ\"], [\"online\", \"online\", \"JJ\"], [\"training\", \"training\", \"NN\"], [\",\", \",\", \".\"], [\"in\", \"in\", \"IN\"], [\"addition\", \"addition\", \"NN\"], [\"to\", \"to\", \"TO\"], [\"live\", \"live\", \"VB\"], [\"in\", \"in\", \"IN\"], [\"-\", \"-\", \".\"], [\"person\", \"person\", \"NN\"], [\"training\", \"training\", \"NN\"], [\"at\", \"at\", \"IN\"], [\"conferences\", \"conference\", \"NNS\"], [\"and\", \"and\", \"CC\"], [\"other\", \"other\", \"JJ\"], [\"locations\", \"location\", \"NNS\"], [\".\", \".\", \".\"]]',\n"," '[[\"But\", \"but\", \"CC\"], [\"the\", \"the\", \"DT\"], [\"same\", \"same\", \"JJ\"], [\"standard\", \"standard\", \"JJ\"], [\"applies\", \"apply\", \"VBZ\"], [\":\", \":\", \".\"], [\"we\", \"we\", \"PRP\"], [\"want\", \"want\", \"VBP\"], [\"the\", \"the\", \"DT\"], [\"people\", \"people\", \"NNS\"], [\"who\", \"who\", \"WP\"], [\"learn\", \"learn\", \"VBP\"], [\"with\", \"with\", \"IN\"], [\"us\", \"us\", \"PRP\"], [\"to\", \"to\", \"TO\"], [\"feel\", \"feel\", \"VB\"], [\"like\", \"like\", \"IN\"], [\"they\", \"they\", \"PRP\"], [\"have\", \"have\", \"VBP\"], [\"an\", \"an\", \"DT\"], [\"expert\", \"expert\", \"NN\"], [\"looking\", \"look\", \"VBG\"], [\"over\", \"over\", \"IN\"], [\"their\", \"their\", \"PRP$\"], [\"shoulders\", \"shoulder\", \"NNS\"], [\"and\", \"and\", \"CC\"], [\"providing\", \"provide\", \"VBG\"], [\"seasoned\", \"seasoned\", \"JJ\"], [\"advice\", \"advice\", \"NN\"], [\".\", \".\", \".\"]]',\n"," '[[\"How\", \"how\", \"WRB\"], [\"do\", \"do\", \"VBP\"], [\"we\", \"we\", \"PRP\"], [\"do\", \"do\", \"VB\"], [\"that\", \"that\", \"DT\"], [\"?\", \"?\", \".\"]]',\n"," '[[\"It\", \"it\", \"PRP\"], [\"starts\", \"start\", \"VBZ\"], [\"with\", \"with\", \"IN\"], [\"realizing\", \"realize\", \"VBG\"], [\"that\", \"that\", \"IN\"], [\"learning\", \"learning\", \"NN\"], [\"is\", \"is\", \"VBZ\"], [\"n\\'t\", \"n\\'t\", \"RB\"], [\"just\", \"just\", \"RB\"], [\"a\", \"a\", \"DT\"], [\"one\", \"one\", \"CD\"], [\"-\", \"-\", \".\"], [\"way\", \"way\", \"NN\"], [\"experience\", \"experience\", \"NN\"], [\".\", \".\", \".\"]]',\n"," '[[\"When\", \"when\", \"WRB\"], [\"we\", \"we\", \"PRP\"], [\"analyzed\", \"analyze\", \"VBD\"], [\"conference\", \"conference\", \"NN\"], [\"attendance\", \"attendance\", \"NN\"], [\"and\", \"and\", \"CC\"], [\"early\", \"early\", \"JJ\"], [\"online\", \"online\", \"JJ\"], [\"courses\", \"course\", \"NNS\"], [\",\", \",\", \".\"], [\"we\", \"we\", \"PRP\"], [\"saw\", \"see\", \"VBD\"], [\"that\", \"that\", \"IN\"], [\"people\", \"people\", \"NNS\"], [\"participate\", \"participate\", \"VBP\"], [\"as\", \"as\", \"IN\"], [\"groups\", \"group\", \"NNS\"], [\".\", \".\", \".\"]]',\n"," '[[\"More\", \"more\", \"JJR\"], [\"than\", \"than\", \"IN\"], [\"50\", \"50\", \"CD\"], [\"%\", \"%\", \"NN\"], [\"attend\", \"attend\", \"VBP\"], [\"our\", \"our\", \"PRP$\"], [\"online\", \"online\", \"JJ\"], [\"courses\", \"course\", \"NNS\"], [\"as\", \"as\", \"IN\"], [\"teams\", \"team\", \"NNS\"], [\".\", \".\", \".\"]]',\n"," '[[\"People\", \"People\", \"NNS\"], [\"do\", \"do\", \"VBP\"], [\"n\\'t\", \"n\\'t\", \"RB\"], [\"attend\", \"attend\", \"VB\"], [\"courses\", \"course\", \"NNS\"], [\"as\", \"as\", \"IN\"], [\"teams\", \"team\", \"NNS\"], [\"just\", \"just\", \"RB\"], [\"so\", \"so\", \"IN\"], [\"they\", \"they\", \"PRP\"], [\"can\", \"can\", \"MD\"], [\"hang\", \"hang\", \"VB\"], [\"out\", \"out\", \"RP\"], [\"together\", \"together\", \"RB\"], [\"during\", \"during\", \"IN\"], [\"breaks\", \"break\", \"NNS\"], [\";\", \";\", \".\"], [\"they\", \"they\", \"PRP\"], [\"attend\", \"attend\", \"VBP\"], [\"as\", \"as\", \"IN\"], [\"teams\", \"team\", \"NNS\"], [\"so\", \"so\", \"IN\"], [\"they\", \"they\", \"PRP\"], [\"can\", \"can\", \"MD\"], [\"learn\", \"learn\", \"VB\"], [\"from\", \"from\", \"IN\"], [\"each\", \"each\", \"DT\"], [\"other\", \"other\", \"JJ\"], [\",\", \",\", \".\"], [\"apply\", \"apply\", \"VB\"], [\"their\", \"their\", \"PRP$\"], [\"new\", \"new\", \"JJ\"], [\"knowledge\", \"knowledge\", \"NN\"], [\"to\", \"to\", \"IN\"], [\"their\", \"their\", \"PRP$\"], [\"particular\", \"particular\", \"JJ\"], [\"situation\", \"situation\", \"NN\"], [\",\", \",\", \".\"], [\"and\", \"and\", \"CC\"], [\"bring\", \"bring\", \"VB\"], [\"that\", \"that\", \"DT\"], [\"knowledge\", \"knowledge\", \"NN\"], [\"back\", \"back\", \"RB\"], [\"to\", \"to\", \"IN\"], [\"their\", \"their\", \"PRP$\"], [\"co\", \"co\", \"NNS\"], [\"-\", \"-\", \"NNS\"], [\"workers\", \"worker\", \"NNS\"], [\".\", \".\", \".\"]]',\n"," '[[\"When\", \"when\", \"WRB\"], [\"a\", \"a\", \"DT\"], [\"team\", \"team\", \"NN\"], [\"attends\", \"attend\", \"VBZ\"], [\"training\", \"training\", \"NN\"], [\",\", \",\", \".\"], [\"the\", \"the\", \"DT\"], [\"group\", \"group\", \"NN\"], [\"learns\", \"learn\", \"VBZ\"], [\"much\", \"much\", \"RB\"], [\"more\", \"more\", \"JJR\"], [\"than\", \"than\", \"IN\"], [\"sum\", \"sum\", \"NN\"], [\"of\", \"of\", \"IN\"], [\"the\", \"the\", \"DT\"], [\"individual\", \"individual\", \"JJ\"], [\"experiences\", \"experience\", \"NNS\"], [\".\", \".\", \".\"]]']"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["%sx more a4.json"]},{"cell_type":"code","execution_count":null,"id":"embedded-bubble","metadata":{"id":"embedded-bubble"},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"located-journal","metadata":{"editable":true,"id":"located-journal"},"source":["### Example: TF-IDF"]},{"cell_type":"markdown","id":"impaired-reggae","metadata":{"editable":true,"id":"impaired-reggae"},"source":["Here we use results from our parsing to calculate a <font color='blue'>term frequency - inverse document frequency (TF-IDF)</font> metric to construct **feature vectors** per document. First we'll load a **stopword** list, for common words to ignore from the analysis:"]},{"cell_type":"code","execution_count":null,"id":"monthly-providence","metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"id":"monthly-providence","outputId":"3dab1cc6-9183-4a92-cf77-cabd29f38ccb"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'i', 'for', 'about', 'my', 'this', 'some', 'their', 'we', 'handle', 'you', 'since', 'the', 'which', 'come', 'of', 'or', 'one', 'all', 'up', 'find', 'that', 'an', 'each', 'as', 'try', 'now', 'where', 'see', 'such', 'other', 'feel', 'a', 'have', 'it', 'like', 'around', 'next', 'what', 'much', 'with', 'on', 'get', 'not', 'take', 'how', 'at', 'us', 'go', 'but', 'more', 'use', 'few', 'both', \"n't\", 'by', 'its', 'our', 'there', 'who', 'two', 'write', 'over', 'can', 'when', 'be', 'let', 'out', 'your', 'they', 'in', 'same', 'if', 'and', 'new', 'just', 'to', 'want', 'so', 'then', 'than', 'while', 'from', 'do', 'also'}\n"]}],"source":["import pynlp\n","\n","stopwords = pynlp.load_stopwords(\"stop.txt\")\n","print(stopwords)"]},{"cell_type":"markdown","id":"boxed-timothy","metadata":{"editable":true,"id":"boxed-timothy"},"source":["Next, we'll use a function from our `pynlp` library to iterate through the keywords for one of the parsed HTML documents:"]},{"cell_type":"code","execution_count":null,"id":"knowing-crossing","metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"id":"knowing-crossing","outputId":"3be4cd47-ba82-40df-89e4-6dc066930479"},"outputs":[{"data":{"text/plain":["['a1.json', 'a2.json', 'a3.json', 'a4.json']"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["%sx ls *.json"]},{"cell_type":"code","execution_count":null,"id":"tropical-harrison","metadata":{"collapsed":true,"editable":true,"jupyter":{"outputs_hidden":true},"id":"tropical-harrison","outputId":"bd629ed5-95c8-4154-8783-b04adcc4d7c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["WordNode(raw='Almost', root='almost', pos='RB')\n","WordNode(raw='a', root='a', pos='DT')\n","WordNode(raw='year', root='year', pos='NN')\n","WordNode(raw='ago', root='ago', pos='RB')\n","WordNode(raw=',', root=',', pos='.')\n","WordNode(raw='we', root='we', pos='PRP')\n","WordNode(raw='published', root='publish', pos='VBD')\n","WordNode(raw='our', root='our', pos='PRP$')\n","WordNode(raw='now', root='now', pos='RB')\n","WordNode(raw='-', root='-', pos='.')\n","WordNode(raw='annual', root='annual', pos='JJ')\n","WordNode(raw='landscape', root='landscape', pos='NN')\n","WordNode(raw='of', root='of', pos='IN')\n","WordNode(raw='machine', root='machine', pos='NN')\n","WordNode(raw='intelligence', root='intelligence', pos='NN')\n","WordNode(raw='companies', root='company', pos='NNS')\n","WordNode(raw=',', root=',', pos='.')\n","WordNode(raw='and', root='and', pos='CC')\n","WordNode(raw='goodness', root='goodness', pos='NN')\n","WordNode(raw='have', root='have', pos='VBP')\n","WordNode(raw='we', root='we', pos='PRP')\n","WordNode(raw='seen', root='see', pos='VBN')\n","WordNode(raw='a', root='a', pos='DT')\n","WordNode(raw='lot', root='lot', pos='NN')\n","WordNode(raw='of', root='of', pos='IN')\n","WordNode(raw='activity', root='activity', pos='NN')\n","WordNode(raw='since', root='since', pos='IN')\n","WordNode(raw='then', root='then', pos='RB')\n","WordNode(raw='.', root='.', pos='.')\n","WordNode(raw='This', root='this', pos='DT')\n","WordNode(raw='year', root='year', pos='NN')\n","WordNode(raw=\"'s\", root=\"'s\", pos='POS')\n","WordNode(raw='landscape', root='landscape', pos='NN')\n","WordNode(raw='has', root='has', pos='VBZ')\n","WordNode(raw='a', root='a', pos='DT')\n","WordNode(raw='third', root='third', pos='RB')\n","WordNode(raw='more', root='more', pos='JJR')\n","WordNode(raw='companies', root='company', pos='NNS')\n","WordNode(raw='than', root='than', pos='IN')\n","WordNode(raw='our', root='our', pos='PRP$')\n","WordNode(raw='first', root='first', pos='JJ')\n","WordNode(raw='one', root='one', pos='NN')\n","WordNode(raw='did', root='did', pos='VBD')\n","WordNode(raw='two', root='two', pos='CD')\n","WordNode(raw='years', root='year', pos='NNS')\n","WordNode(raw='ago', root='ago', pos='RB')\n","WordNode(raw=',', root=',', pos='.')\n","WordNode(raw='and', root='and', pos='CC')\n","WordNode(raw='it', root='it', pos='PRP')\n","WordNode(raw='feels', root='feel', pos='VBZ')\n","WordNode(raw='even', root='even', pos='RB')\n","WordNode(raw='more', root='more', pos='RBR')\n","WordNode(raw='futile', root='futile', pos='JJ')\n","WordNode(raw='to', root='to', pos='TO')\n","WordNode(raw='try', root='try', pos='VB')\n","WordNode(raw='to', root='to', pos='TO')\n","WordNode(raw='be', root='be', pos='VB')\n","WordNode(raw='comprehensive', root='comprehensive', pos='JJ')\n","WordNode(raw=',', root=',', pos='.')\n","WordNode(raw='since', root='since', pos='IN')\n","WordNode(raw='this', root='this', pos='DT')\n","WordNode(raw='just', root='just', pos='RB')\n","WordNode(raw='scratches', root='scratch', pos='VBZ')\n","WordNode(raw='the', root='the', pos='DT')\n","WordNode(raw='surface', root='surface', pos='NN')\n","WordNode(raw='of', root='of', pos='IN')\n","WordNode(raw='all', root='all', pos='DT')\n","WordNode(raw='of', root='of', pos='IN')\n","WordNode(raw='the', root='the', pos='DT')\n","WordNode(raw='activity', root='activity', pos='NN')\n","WordNode(raw='out', root='out', pos='RB')\n","WordNode(raw='there', root='there', pos='RB')\n","WordNode(raw='.', root='.', pos='.')\n","WordNode(raw='As', root='as', pos='IN')\n","WordNode(raw='has', root='has', pos='VBZ')\n","WordNode(raw='been', root='been', pos='VBN')\n","WordNode(raw='the', root='the', pos='DT')\n","WordNode(raw='case', root='case', pos='NN')\n","WordNode(raw='for', root='for', pos='IN')\n","WordNode(raw='the', root='the', pos='DT')\n","WordNode(raw='last', root='last', pos='JJ')\n","WordNode(raw='couple', root='couple', pos='NN')\n","WordNode(raw='of', root='of', pos='IN')\n","WordNode(raw='years', root='year', pos='NNS')\n","WordNode(raw=',', root=',', pos='.')\n","WordNode(raw='our', root='our', pos='PRP$')\n","WordNode(raw='fund', root='fund', pos='NN')\n","WordNode(raw='still', root='still', pos='RB')\n","WordNode(raw='obsesses', root='obsess', pos='VBZ')\n","WordNode(raw='over', root='over', pos='IN')\n","WordNode(raw='\"', root='\"', pos='.')\n","WordNode(raw='problem', root='problem', pos='VB')\n","WordNode(raw='first', root='first', pos='RB')\n","WordNode(raw='\"', root='\"', pos='.')\n","WordNode(raw='machine', root='machine', pos='NN')\n","WordNode(raw='intelligence', root='intelligence', pos='NN')\n","WordNode(raw='--', root='--', pos='.')\n","WordNode(raw='we', root='we', pos='PRP')\n","WordNode(raw=\"'ve\", root=\"'ve\", pos='VB')\n","WordNode(raw='invested', root='invest', pos='VBN')\n","WordNode(raw='in', root='in', pos='IN')\n","WordNode(raw='35', root='35', pos='CD')\n","WordNode(raw='machine', root='machine', pos='NN')\n","WordNode(raw='intelligence', root='intelligence', pos='NN')\n","WordNode(raw='companies', root='company', pos='NNS')\n","WordNode(raw='solving', root='solve', pos='VBG')\n","WordNode(raw='35', root='35', pos='CD')\n","WordNode(raw='meaningful', root='meaningful', pos='JJ')\n","WordNode(raw='problems', root='problem', pos='NNS')\n","WordNode(raw='in', root='in', pos='IN')\n","WordNode(raw='areas', root='area', pos='NNS')\n","WordNode(raw='from', root='from', pos='IN')\n","WordNode(raw='security', root='security', pos='NN')\n","WordNode(raw='to', root='to', pos='IN')\n","WordNode(raw='recruiting', root='recruit', pos='VBG')\n","WordNode(raw='to', root='to', pos='IN')\n","WordNode(raw='software', root='software', pos='NN')\n","WordNode(raw='development', root='development', pos='NN')\n","WordNode(raw='.', root='.', pos='.')\n","WordNode(raw='(', root='(', pos='.')\n","WordNode(raw='Our', root='our', pos='PRP$')\n","WordNode(raw='fund', root='fund', pos='NN')\n","WordNode(raw='focuses', root='focus', pos='VBZ')\n","WordNode(raw='on', root='on', pos='IN')\n","WordNode(raw='the', root='the', pos='DT')\n","WordNode(raw='future', root='future', pos='NN')\n","WordNode(raw='of', root='of', pos='IN')\n","WordNode(raw='work', root='work', pos='NN')\n","WordNode(raw=',', root=',', pos='.')\n","WordNode(raw='so', root='so', pos='CC')\n","WordNode(raw='there', root='there', pos='EX')\n","WordNode(raw='are', root='are', pos='VBP')\n","WordNode(raw='some', root='some', pos='DT')\n","WordNode(raw='machine', root='machine', pos='NN')\n","WordNode(raw='intelligence', root='intelligence', pos='NN')\n","WordNode(raw='domains', root='domain', pos='NNS')\n","WordNode(raw='where', root='where', pos='WRB')\n","WordNode(raw='we', root='we', pos='PRP')\n","WordNode(raw='invest', root='invest', pos='VBP')\n","WordNode(raw='more', root='more', pos='JJR')\n","WordNode(raw='than', root='than', pos='IN')\n","WordNode(raw='others', root='other', pos='NNS')\n","WordNode(raw='.', root='.', pos='.')\n","WordNode(raw=')', root=')', pos='.')\n","WordNode(raw='At', root='at', pos='IN')\n","WordNode(raw='the', root='the', pos='DT')\n","WordNode(raw='same', root='same', pos='JJ')\n","WordNode(raw='time', root='time', pos='NN')\n","WordNode(raw=',', root=',', pos='.')\n","WordNode(raw='the', root='the', pos='DT')\n","WordNode(raw='hype', root='hype', pos='NN')\n","WordNode(raw='around', root='around', pos='IN')\n","WordNode(raw='machine', root='machine', pos='NN')\n","WordNode(raw='intelligence', root='intelligence', pos='NN')\n","WordNode(raw='methods', root='method', pos='NNS')\n","WordNode(raw='continues', root='continue', pos='VBZ')\n","WordNode(raw='to', root='to', pos='TO')\n","WordNode(raw='grow', root='grow', pos='VB')\n","WordNode(raw=':', root=':', pos='.')\n","WordNode(raw='the', root='the', pos='DT')\n","WordNode(raw='words', root='word', pos='NNS')\n","WordNode(raw='\"', root='\"', pos='.')\n","WordNode(raw='deep', root='deep', pos='JJ')\n","WordNode(raw='learning', root='learning', pos='NN')\n","WordNode(raw='\"', root='\"', pos='.')\n","WordNode(raw='now', root='now', pos='RB')\n","WordNode(raw='equally', root='equally', pos='RB')\n","WordNode(raw='represent', root='represent', pos='VBP')\n","WordNode(raw='a', root='a', pos='DT')\n","WordNode(raw='series', root='series', pos='NN')\n","WordNode(raw='of', root='of', pos='IN')\n","WordNode(raw='meaningful', root='meaningful', pos='JJ')\n","WordNode(raw='breakthroughs', root='breakthrough', pos='NNS')\n","WordNode(raw='(', root='(', pos='.')\n","WordNode(raw='wonderful', root='wonderful', pos='JJ')\n","WordNode(raw=')', root=')', pos='.')\n","WordNode(raw='but', root='but', pos='CC')\n","WordNode(raw='also', root='also', pos='RB')\n","WordNode(raw='a', root='a', pos='DT')\n","WordNode(raw='hyped', root='hype', pos='VBN')\n","WordNode(raw='phrase', root='phrase', pos='NN')\n","WordNode(raw='like', root='like', pos='IN')\n","WordNode(raw='\"', root='\"', pos='.')\n","WordNode(raw='big', root='big', pos='JJ')\n","WordNode(raw='data', root='datum', pos='NNS')\n","WordNode(raw='\"', root='\"', pos='.')\n","WordNode(raw='(', root='(', pos='.')\n","WordNode(raw='not', root='not', pos='RB')\n","WordNode(raw='so', root='so', pos='RB')\n","WordNode(raw='good', root='good', pos='JJ')\n","WordNode(raw='!', root='!', pos='.')\n","WordNode(raw=')', root=')', pos='.')\n","WordNode(raw='.', root='.', pos='.')\n","WordNode(raw='We', root='we', pos='PRP')\n","WordNode(raw='care', root='care', pos='VBP')\n","WordNode(raw='about', root='about', pos='IN')\n","WordNode(raw='whether', root='whether', pos='IN')\n","WordNode(raw='a', root='a', pos='DT')\n","WordNode(raw='founder', root='founder', pos='NN')\n","WordNode(raw='uses', root='use', pos='VBZ')\n","WordNode(raw='the', root='the', pos='DT')\n","WordNode(raw='right', root='right', pos='JJ')\n","WordNode(raw='method', root='method', pos='NN')\n","WordNode(raw='to', root='to', pos='TO')\n","WordNode(raw='solve', root='solve', pos='VB')\n","WordNode(raw='a', root='a', pos='DT')\n","WordNode(raw='problem', root='problem', pos='NN')\n","WordNode(raw=',', root=',', pos='.')\n","WordNode(raw='not', root='not', pos='RB')\n","WordNode(raw='the', root='the', pos='DT')\n","WordNode(raw='fanciest', root='fanciest', pos='JJ')\n","WordNode(raw='one', root='one', pos='CD')\n","WordNode(raw='.', root='.', pos='.')\n","WordNode(raw='We', root='we', pos='PRP')\n","WordNode(raw='favor', root='favor', pos='VBP')\n","WordNode(raw='those', root='those', pos='DT')\n","WordNode(raw='who', root='who', pos='WP')\n","WordNode(raw='apply', root='apply', pos='VBP')\n","WordNode(raw='technology', root='technology', pos='NN')\n","WordNode(raw='thoughtfully', root='thoughtfully', pos='RB')\n","WordNode(raw='.', root='.', pos='.')\n","WordNode(raw='What', root='what', pos='WP')\n","WordNode(raw=\"'s\", root=\"'s\", pos='VBZ')\n","WordNode(raw='the', root='the', pos='DT')\n","WordNode(raw='biggest', root='biggest', pos='JJS')\n","WordNode(raw='change', root='change', pos='NN')\n","WordNode(raw='in', root='in', pos='IN')\n","WordNode(raw='the', root='the', pos='DT')\n","WordNode(raw='last', root='last', pos='JJ')\n","WordNode(raw='year', root='year', pos='NN')\n","WordNode(raw='?', root='?', pos='.')\n","WordNode(raw='We', root='we', pos='PRP')\n","WordNode(raw='are', root='are', pos='VBP')\n","WordNode(raw='getting', root='get', pos='VBG')\n","WordNode(raw='inbound', root='inbound', pos='JJ')\n","WordNode(raw='inquiries', root='inquiry', pos='NNS')\n","WordNode(raw='from', root='from', pos='IN')\n","WordNode(raw='a', root='a', pos='DT')\n","WordNode(raw='different', root='different', pos='JJ')\n","WordNode(raw='mix', root='mix', pos='NN')\n","WordNode(raw='of', root='of', pos='IN')\n","WordNode(raw='people', root='people', pos='NNS')\n","WordNode(raw='.', root='.', pos='.')\n","WordNode(raw='For', root='for', pos='IN')\n","WordNode(raw='v1.0', root='v1.0', pos='NNP')\n","WordNode(raw=',', root=',', pos='.')\n","WordNode(raw='we', root='we', pos='PRP')\n","WordNode(raw='heard', root='hear', pos='VBD')\n","WordNode(raw='almost', root='almost', pos='RB')\n","WordNode(raw='exclusively', root='exclusively', pos='RB')\n","WordNode(raw='from', root='from', pos='IN')\n","WordNode(raw='founders', root='founder', pos='NNS')\n","WordNode(raw='and', root='and', pos='CC')\n","WordNode(raw='academics', root='academic', pos='NNS')\n","WordNode(raw='.', root='.', pos='.')\n","WordNode(raw='Then', root='then', pos='RB')\n","WordNode(raw='came', root='come', pos='VBD')\n","WordNode(raw='a', root='a', pos='DT')\n","WordNode(raw='healthy', root='healthy', pos='JJ')\n","WordNode(raw='mix', root='mix', pos='NN')\n","WordNode(raw='of', root='of', pos='IN')\n","WordNode(raw='investors', root='investor', pos='NNS')\n","WordNode(raw=',', root=',', pos='.')\n","WordNode(raw='both', root='both', pos='DT')\n","WordNode(raw='private', root='private', pos='JJ')\n","WordNode(raw='and', root='and', pos='CC')\n","WordNode(raw='public', root='public', pos='JJ')\n","WordNode(raw='.', root='.', pos='.')\n","WordNode(raw='Now', root='now', pos='RB')\n","WordNode(raw='overwhelmingly', root='overwhelmingly', pos='RB')\n","WordNode(raw='we', root='we', pos='PRP')\n","WordNode(raw='have', root='have', pos='VBP')\n","WordNode(raw='heard', root='hear', pos='VBN')\n","WordNode(raw='from', root='from', pos='IN')\n","WordNode(raw='existing', root='exist', pos='VBG')\n","WordNode(raw='companies', root='company', pos='NNS')\n","WordNode(raw='trying', root='try', pos='VBG')\n","WordNode(raw='to', root='to', pos='TO')\n","WordNode(raw='figure', root='figure', pos='VB')\n","WordNode(raw='out', root='out', pos='RP')\n","WordNode(raw='how', root='how', pos='WRB')\n","WordNode(raw='to', root='to', pos='TO')\n","WordNode(raw='transform', root='transform', pos='VB')\n","WordNode(raw='their', root='their', pos='PRP$')\n","WordNode(raw='businesses', root='business', pos='NNS')\n","WordNode(raw='using', root='use', pos='VBG')\n","WordNode(raw='machine', root='machine', pos='NN')\n","WordNode(raw='intelligence', root='intelligence', pos='NN')\n","WordNode(raw='.', root='.', pos='.')\n"]}],"source":["json_file = \"a1.json\"\n","\n","for lex in pynlp.lex_iter(json_file):\n","    print(lex)"]},{"cell_type":"markdown","id":"following-creek","metadata":{"editable":true,"id":"following-creek"},"source":["We need to initialize some data structures for counting keywords. BTW, if you've heard about how Big Data projects use [word count](http://spark.apache.org/examples.html) programs to demonstrate their capabilities, here's a major use case for that. \n","\n","Even so, our examples are conceptually simple, built for relatively small files, and are not intended to scale:"]},{"cell_type":"code","execution_count":null,"id":"aggressive-maker","metadata":{"editable":true,"id":"aggressive-maker"},"outputs":[],"source":["from collections import defaultdict\n","\n","files = [\"a4.json\", \"a3.json\", \"a2.json\", \"a1.json\"]\n","files_tf = {}\n","\n","d = len(files)\n","df = defaultdict(int)"]},{"cell_type":"markdown","id":"chronic-express","metadata":{"editable":true,"id":"chronic-express"},"source":["Iterate through each parsed file, tallying counts for `tf` for each document while also tallying counts for `df` across all documents:"]},{"cell_type":"code","execution_count":null,"id":"purple-exhaust","metadata":{"collapsed":true,"editable":true,"jupyter":{"outputs_hidden":true},"id":"purple-exhaust","outputId":"31c00ea3-d661-4d9b-9b0f-e2ffed2bc0e2"},"outputs":[{"name":"stdout","output_type":"stream","text":["a1.json defaultdict(<class 'int'>, {'almost': 2, 'year': 5, 'ago': 2, 'publish': 1, 'annual': 1, 'landscape': 2, 'machine': 6, 'intelligence': 6, 'company': 4, 'goodness': 1, 'lot': 1, 'activity': 2, \"'s\": 2, 'has': 2, 'third': 1, 'first': 2, 'did': 1, 'even': 1, 'futile': 1, 'comprehensive': 1, 'scratch': 1, 'surface': 1, 'been': 1, 'case': 1, 'last': 2, 'couple': 1, 'fund': 2, 'still': 1, 'obsess': 1, 'problem': 3, \"'ve\": 1, 'invest': 2, '35': 2, 'solve': 2, 'meaningful': 2, 'area': 1, 'security': 1, 'recruit': 1, 'software': 1, 'development': 1, 'focus': 1, 'future': 1, 'work': 1, 'are': 2, 'domain': 1, 'time': 1, 'hype': 2, 'method': 2, 'continue': 1, 'grow': 1, 'word': 1, 'deep': 1, 'learning': 1, 'equally': 1, 'represent': 1, 'series': 1, 'breakthrough': 1, 'wonderful': 1, 'phrase': 1, 'big': 1, 'datum': 1, 'good': 1, 'care': 1, 'whether': 1, 'founder': 2, 'right': 1, 'fanciest': 1, 'favor': 1, 'those': 1, 'apply': 1, 'technology': 1, 'thoughtfully': 1, 'biggest': 1, 'change': 1, 'inbound': 1, 'inquiry': 1, 'different': 1, 'mix': 2, 'people': 1, 'v1.0': 1, 'hear': 2, 'exclusively': 1, 'academic': 1, 'healthy': 1, 'investor': 1, 'private': 1, 'public': 1, 'overwhelmingly': 1, 'exist': 1, 'figure': 1, 'transform': 1, 'business': 1})\n"]}],"source":["for json_file in files:\n","    tf = defaultdict(int)\n","\n","    for lex in pynlp.lex_iter(json_file):\n","        if (lex.pos != \".\") and (lex.root not in stopwords):\n","            tf[lex.root] += 1\n","\n","    files_tf[json_file] = tf\n","\n","    for word in tf.keys():\n","        df[word] += 1\n","\n","## print results for just the last file in the sequence\n","print(json_file, files_tf[json_file])"]},{"cell_type":"markdown","id":"sought-palestinian","metadata":{"editable":true,"id":"sought-palestinian"},"source":["Let's take a look at the `df` results overall. If there are low-information common words in the list that you'd like to filter out, move them to your **stopword** list."]},{"cell_type":"code","execution_count":null,"id":"geographic-michael","metadata":{"collapsed":true,"editable":true,"jupyter":{"outputs_hidden":true},"id":"geographic-michael","outputId":"0e121e33-287f-4991-eecc-e55e9214dda4"},"outputs":[{"name":"stdout","output_type":"stream","text":["learning 4\n","'s 4\n","has 4\n","learn 3\n","been 3\n","was 3\n","year 3\n","time 3\n","machine 3\n","is 2\n","interact 2\n","student 2\n","step 2\n","feedback 2\n","whether 2\n","o'reilly 2\n","company 2\n","publish 2\n","book 2\n","were 2\n","provide 2\n","'ve 2\n","past 2\n","training 2\n","apply 2\n","people 2\n","start 2\n","attendance 2\n","early 2\n","deep 2\n","problem 2\n","post 2\n","become 2\n","thing 2\n","local 2\n","are 2\n","many 2\n","exist 2\n","programming 2\n","solve 2\n","biggest 2\n","open 2\n","first 2\n","datum 2\n","work 2\n","shot 1\n","process 1\n","course 1\n","pass 1\n","exam 1\n","instructor 1\n","assess 1\n","progress 1\n","plan 1\n","ongoing 1\n","loop 1\n","involve 1\n","everyone 1\n","classroom 1\n","virtual 1\n","physical 1\n","media 1\n","always 1\n","begin 1\n","mid-'80s 1\n","editorial 1\n","guidance 1\n","author 1\n","friend 1\n","look 1\n","reader 1\n","shoulder 1\n","wise 1\n","experienced 1\n","advice 1\n","2016 1\n","'re 1\n","long 1\n","instructional 1\n","video 1\n","conference 1\n","introduce 1\n","live 1\n","online 1\n","addition 1\n","person 1\n","location 1\n","standard 1\n","expert 1\n","seasoned 1\n","realize 1\n","way 1\n","experience 1\n","analyze 1\n","participate 1\n","group 1\n","50 1\n","% 1\n","attend 1\n","team 1\n","People 1\n","hang 1\n","together 1\n","during 1\n","break 1\n","knowledge 1\n","particular 1\n","situation 1\n","bring 1\n","back 1\n","co 1\n","- 1\n","worker 1\n","sum 1\n","individual 1\n","heart 1\n","lie 1\n","hard 1\n","optimization 1\n","several 1\n","decade 1\n","after 1\n","introduction 1\n","neural 1\n","network 1\n","difficulty 1\n","barrier 1\n","mainstream 1\n","usage 1\n","contribute 1\n","  1\n","decline 1\n","1990 1\n","2000 1\n","overcome 1\n","issue 1\n","explore 1\n","hardness 1\n","optimize 1\n","theory 1\n","say 1\n","nutshell 1\n","deeper 1\n","harder 1\n","simplest 1\n","single 1\n","node 1\n","perceptron 1\n","whose 1\n","convex 1\n","nice 1\n","minimum 1\n","global 1\n","minima 1\n","rich 1\n","variety 1\n","algorithm 1\n","every 1\n","better 1\n","polynomial 1\n","discover 1\n","weight 1\n","neuron 1\n","easy 1\n","graphic 1\n","below 1\n","happen 1\n","natural 1\n","add 1\n","keep 1\n","layer 1\n","n 1\n","edge 1\n","correctly 1\n","classify 1\n","give 1\n","set 1\n","linear 1\n","special 1\n","subset 1\n","question 1\n","arise 1\n","make 1\n","similar 1\n","guarantee 1\n","unfortunately 1\n","provably 1\n","general 1\n","would 1\n","necessary 1\n","hit 1\n","computer 1\n","science 1\n","think 1\n","hope 1\n","researcher 1\n","optimal 1\n","because 1\n","np 1\n","mean 1\n","thousand 1\n","indeed 1\n","1988 1\n","j. 1\n","stephen 1\n","judd 1\n","show 1\n","follow 1\n","spark 1\n","version 1\n","0.4 1\n","0.5 1\n","release 1\n","hive 1\n","pig 1\n","processing 1\n","evaluate 1\n","mahout 1\n","being 1\n","bit 1\n","resistant 1\n","language 1\n","scala 1\n","later 1\n","love 1\n","immediately 1\n","user 1\n","fan 1\n","speed 1\n","gain 1\n","addict 1\n","amplab 1\n","roll 1\n","useful 1\n","example 1\n","library 1\n","steady 1\n","pace 1\n","soon 1\n","myself 1\n","reason 1\n","task 1\n","project 1\n","developer 1\n","meetup 1\n","important 1\n","professors 1\n","mid 1\n","2012 1\n","san 1\n","francisco 1\n","audience 1\n","preview 1\n","streaming 1\n","remember 1\n","reaction 1\n","presentation 1\n","very 1\n","clearly 1\n","immediate 1\n","interest 1\n","enthusiasm 1\n","clear 1\n","me 1\n","popular 1\n","storm 1\n","prospect 1\n","simplify 1\n","infrastructure 1\n","due 1\n","ability 1\n","batch 1\n","attractive 1\n","broach 1\n","idea 1\n","matei 1\n","zaharia 1\n","creator 1\n","initial 1\n","conversation 1\n","lead 1\n","title 1\n","fall 1\n","fortunate 1\n","enough 1\n","invite 1\n","amp 1\n","camp 1\n","enroute 1\n","event 1\n","seven 1\n","why 1\n","camps 1\n","combine 1\n","talk 1\n","well 1\n","hand 1\n","tutorial 1\n","day 1\n","defacto 1\n","community 1\n","gather 1\n","stand 1\n","cloud 1\n","friendly 1\n","beginning 1\n","tool 1\n","help 1\n","play 1\n","aws 1\n","second 1\n","unveiling 1\n","pyspark 1\n","most 1\n","had 1\n","jvm 1\n","java 1\n","clojure 1\n","background 1\n","large 1\n","number 1\n","scientist 1\n","python 1\n","primary 1\n","extremely 1\n","recent 1\n","survey 1\n","suggest 1\n","finally 1\n","feature 1\n","prominently 1\n","include 1\n","draw 1\n","potential 1\n","almost 1\n","ago 1\n","annual 1\n","landscape 1\n","intelligence 1\n","goodness 1\n","lot 1\n","activity 1\n","third 1\n","did 1\n","even 1\n","futile 1\n","comprehensive 1\n","scratch 1\n","surface 1\n","case 1\n","last 1\n","couple 1\n","fund 1\n","still 1\n","obsess 1\n","invest 1\n","35 1\n","meaningful 1\n","area 1\n","security 1\n","recruit 1\n","software 1\n","development 1\n","focus 1\n","future 1\n","domain 1\n","hype 1\n","method 1\n","continue 1\n","grow 1\n","word 1\n","equally 1\n","represent 1\n","series 1\n","breakthrough 1\n","wonderful 1\n","phrase 1\n","big 1\n","good 1\n","care 1\n","founder 1\n","right 1\n","fanciest 1\n","favor 1\n","those 1\n","technology 1\n","thoughtfully 1\n","change 1\n","inbound 1\n","inquiry 1\n","different 1\n","mix 1\n","v1.0 1\n","hear 1\n","exclusively 1\n","academic 1\n","healthy 1\n","investor 1\n","private 1\n","public 1\n","overwhelmingly 1\n","figure 1\n","transform 1\n","business 1\n"]}],"source":["for word, count in sorted(df.items(), key=lambda kv: kv[1], reverse=True):\n","  print(word, count)"]},{"cell_type":"markdown","id":"still-implement","metadata":{"editable":true,"id":"still-implement"},"source":["Finally, we make a second pass through the data, using the `df` counts to normalize `tf` counts, calculating the `tfidf` metrics for each keyword:"]},{"cell_type":"code","execution_count":null,"id":"solid-consultation","metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"id":"solid-consultation"},"outputs":[],"source":["import math\n","\n","for json_file in files:\n","    tf = files_tf[json_file]\n","    keywords = []\n","\n","    for word, count in tf.items():\n","        # Note the 1 added in tf and idf (to avoid problems with 0 counts)\n","        tfidf = float(count) * math.log((d + 1.0) / (df[word] + 1.0))\n","        keywords.append((json_file, tfidf, word,))"]},{"cell_type":"markdown","id":"overhead-watershed","metadata":{"editable":true,"id":"overhead-watershed"},"source":["Let's take a look at the results for one of the files:"]},{"cell_type":"code","execution_count":null,"id":"necessary-works","metadata":{"collapsed":true,"editable":true,"jupyter":{"outputs_hidden":true},"id":"necessary-works","outputId":"d9cafb6c-5a9b-47df-8a07-32f7b81f6374"},"outputs":[{"name":"stdout","output_type":"stream","text":["a1.json\t 5.4977\tintelligence\n","a1.json\t 2.0433\tcompany\n","a1.json\t 1.8326\talmost\n","a1.json\t 1.8326\tago\n","a1.json\t 1.8326\tlandscape\n","a1.json\t 1.8326\tactivity\n","a1.json\t 1.8326\tlast\n","a1.json\t 1.8326\tfund\n","a1.json\t 1.8326\tinvest\n","a1.json\t 1.8326\t35\n","a1.json\t 1.8326\tmeaningful\n","a1.json\t 1.8326\thype\n","a1.json\t 1.8326\tmethod\n","a1.json\t 1.8326\tfounder\n","a1.json\t 1.8326\tmix\n","a1.json\t 1.8326\thear\n","a1.json\t 1.5325\tproblem\n","a1.json\t 1.3389\tmachine\n","a1.json\t 1.1157\tyear\n","a1.json\t 1.0217\tfirst\n","a1.json\t 1.0217\tsolve\n","a1.json\t 1.0217\tare\n","a1.json\t 0.9163\tannual\n","a1.json\t 0.9163\tgoodness\n","a1.json\t 0.9163\tlot\n","a1.json\t 0.9163\tthird\n","a1.json\t 0.9163\tdid\n","a1.json\t 0.9163\teven\n","a1.json\t 0.9163\tfutile\n","a1.json\t 0.9163\tcomprehensive\n","a1.json\t 0.9163\tscratch\n","a1.json\t 0.9163\tsurface\n","a1.json\t 0.9163\tcase\n","a1.json\t 0.9163\tcouple\n","a1.json\t 0.9163\tstill\n","a1.json\t 0.9163\tobsess\n","a1.json\t 0.9163\tarea\n","a1.json\t 0.9163\tsecurity\n","a1.json\t 0.9163\trecruit\n","a1.json\t 0.9163\tsoftware\n","a1.json\t 0.9163\tdevelopment\n","a1.json\t 0.9163\tfocus\n","a1.json\t 0.9163\tfuture\n","a1.json\t 0.9163\tdomain\n","a1.json\t 0.9163\tcontinue\n","a1.json\t 0.9163\tgrow\n","a1.json\t 0.9163\tword\n","a1.json\t 0.9163\tequally\n","a1.json\t 0.9163\trepresent\n","a1.json\t 0.9163\tseries\n","a1.json\t 0.9163\tbreakthrough\n","a1.json\t 0.9163\twonderful\n","a1.json\t 0.9163\tphrase\n","a1.json\t 0.9163\tbig\n","a1.json\t 0.9163\tgood\n","a1.json\t 0.9163\tcare\n","a1.json\t 0.9163\tright\n","a1.json\t 0.9163\tfanciest\n","a1.json\t 0.9163\tfavor\n","a1.json\t 0.9163\tthose\n","a1.json\t 0.9163\ttechnology\n","a1.json\t 0.9163\tthoughtfully\n","a1.json\t 0.9163\tchange\n","a1.json\t 0.9163\tinbound\n","a1.json\t 0.9163\tinquiry\n","a1.json\t 0.9163\tdifferent\n","a1.json\t 0.9163\tv1.0\n","a1.json\t 0.9163\texclusively\n","a1.json\t 0.9163\tacademic\n","a1.json\t 0.9163\thealthy\n","a1.json\t 0.9163\tinvestor\n","a1.json\t 0.9163\tprivate\n","a1.json\t 0.9163\tpublic\n","a1.json\t 0.9163\toverwhelmingly\n","a1.json\t 0.9163\tfigure\n","a1.json\t 0.9163\ttransform\n","a1.json\t 0.9163\tbusiness\n","a1.json\t 0.5108\tpublish\n","a1.json\t 0.5108\t've\n","a1.json\t 0.5108\twork\n","a1.json\t 0.5108\tdeep\n","a1.json\t 0.5108\tdatum\n","a1.json\t 0.5108\twhether\n","a1.json\t 0.5108\tapply\n","a1.json\t 0.5108\tbiggest\n","a1.json\t 0.5108\tpeople\n","a1.json\t 0.5108\texist\n","a1.json\t 0.2231\tbeen\n","a1.json\t 0.2231\ttime\n","a1.json\t 0.0000\t's\n","a1.json\t 0.0000\thas\n","a1.json\t 0.0000\tlearning\n"]}],"source":["for json_file, tfidf, word in sorted(keywords, key=lambda x: x[1], reverse=True):\n","    print(\"%s\\t%7.4f\\t%s\" % (json_file, tfidf, word))"]},{"cell_type":"markdown","id":"funky-election","metadata":{"editable":true,"id":"funky-election"},"source":["Question: how does that vector of ranked keywords compare with your reading of the text from the HTML file?"]},{"cell_type":"code","execution_count":null,"id":"renewable-phoenix","metadata":{"id":"renewable-phoenix"},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"requested-abortion","metadata":{"editable":true,"id":"requested-abortion"},"source":["### Example: semantic similarity"]},{"cell_type":"markdown","id":"middle-biography","metadata":{"id":"middle-biography"},"source":["Cosine similarity between 2 vectors $a, b$ is easy to define:\n","\n","$$simcos(a,b) = \\frac{a \\cdot b}{||a||x||b||} $$"]},{"cell_type":"markdown","id":"instrumental-canada","metadata":{"editable":true,"id":"instrumental-canada"},"source":["Another similarity measure is Jaccard similarity. \n","\n","We can improve on speed in computing similarities using <font color='blue'>MinHash</font> as an approximation of <font color='blue'>Jaccard similarity</font>. We can create a function to calculate a <font color='blue'>MinHash</font> using package 'datasketch':"]},{"cell_type":"code","execution_count":null,"id":"stock-consistency","metadata":{"id":"stock-consistency"},"outputs":[],"source":["### ! conda install -c conda-forge datasketch --yes\n","# ! pip install datasketch"]},{"cell_type":"code","execution_count":null,"id":"confidential-kentucky","metadata":{"editable":true,"id":"confidential-kentucky"},"outputs":[],"source":["from datasketch import MinHash\n","\n","def mh_digest (data):\n","    mh = MinHash(num_perm=512)\n","\n","    for d in data:\n","        mh.update(d.encode('utf8'))\n","\n","    return mh"]},{"cell_type":"markdown","id":"atlantic-zimbabwe","metadata":{"editable":true,"id":"atlantic-zimbabwe"},"source":["Then we'll iterate through each parsed document, adding the keywords to the MinHash:"]},{"cell_type":"code","execution_count":null,"id":"certain-correction","metadata":{"collapsed":true,"editable":true,"jupyter":{"outputs_hidden":true},"id":"certain-correction","outputId":"3253cb5e-867a-418d-81c9-b92c75e97b6c"},"outputs":[{"name":"stdout","output_type":"stream","text":["a4.json {'learning', 'book', 'classroom', 'group', 'instructor', 'experienced', 'year', 'People', 'situation', 'co', 'progress', 'experience', \"mid-'80s\", 'is', '2016', 'editorial', 'person', 'always', 'exam', \"'re\", 'student', 'participate', 'early', 'involve', 'ongoing', 'course', \"o'reilly\", 'were', 'standard', 'hang', 'was', \"'ve\", 'wise', 'video', 'been', 'media', 'plan', 'feedback', 'conference', '-', 'attendance', 'together', 'way', 'step', 'whether', 'during', 'author', 'has', 'team', 'analyze', 'training', 'physical', 'learn', 'interact', 'attend', 'everyone', 'past', 'provide', 'location', 'particular', 'assess', 'realize', 'pass', 'process', 'apply', 'people', 'bring', 'individual', 'shoulder', 'look', 'virtual', 'online', '%', 'break', 'worker', 'shot', 'loop', 'seasoned', '50', 'introduce', 'long', 'company', 'live', 'friend', 'expert', 'sum', \"'s\", 'start', 'publish', 'begin', 'instructional', 'knowledge', 'reader', 'addition', 'back', 'guidance', 'advice'}\n","a3.json {'learning', 'decade', 'time', 'make', '1990', 'harder', 'nutshell', 'explore', 'year', 'would', 'usage', 'local', 'show', 'add', 'because', 'is', 'node', 'hope', 'natural', 'network', 'general', 'whose', 'heart', 'algorithm', ' ', 'easy', 'judd', 'post', 'optimize', 'thousand', 'arise', 'deep', 'minimum', 'problem', 'was', 'deeper', 'follow', 'subset', 'similar', 'edge', 'say', 'necessary', 'been', 'unfortunately', 'solve', 'convex', 'special', 'provably', 'linear', 'classify', 'overcome', 'computer', 'n', 'open', 'biggest', 'optimization', 'layer', 'simplest', 'step', 'thing', 'correctly', 'indeed', 'below', 'science', 'np', 'has', 'stephen', 'training', 'learn', 'global', 'difficulty', 'perceptron', 'past', 'many', 'happen', 'programming', 'neuron', 'machine', 'hardness', 'hard', 'hit', 'rich', 'contribute', 'set', 'graphic', 'are', 'weight', 'better', 'keep', 'mean', 'question', 'discover', '1988', 'mainstream', 'j.', 'barrier', '2000', 'decline', 'every', 'become', 'minima', 'optimal', 'several', \"'s\", 'neural', 'lie', 'exist', 'theory', 'nice', 'polynomial', 'introduction', 'after', 'single', 'give', 'researcher', 'guarantee', 'variety', 'issue', 'think'}\n","a2.json {'learning', 'time', 'love', 'book', 'batch', 'survey', 'first', 'speed', 'title', 'presentation', 'conversation', 'python', 'version', 'local', 'example', 'idea', 'clearly', 'mid', 'steady', 'fan', 'friendly', 'amp', 'student', 'early', 'unveiling', 'soon', 'second', 'camps', 'beginning', 'addict', \"o'reilly\", 'post', 'most', 'fortunate', 'were', 'important', 'well', 'enthusiasm', 'datum', 'jvm', 'tutorial', 'hand', 'large', 'was', 'due', 'pace', 'very', 'user', 'mahout', 'java', 'feedback', 'day', 'gather', 'draw', '2012', 'attendance', 'preview', 'simplify', 'fall', 'finally', 'hive', 'background', 'spark', 'pyspark', 'enough', 'open', 'play', 'release', 'being', 'attractive', 'lead', 'number', 'extremely', 'include', 'roll', 'scala', 'thing', 'interest', 'prominently', 'useful', 'enroute', 'has', 'library', 'learn', 'defacto', 'camp', 'had', 'interact', 'broach', 'me', '0.5', 'clojure', 'developer', 'programming', 'machine', 'many', 'provide', 'event', 'talk', 'work', 'prospect', 'immediately', 'initial', 'recent', 'task', 'suggest', 'professors', 'streaming', 'gain', 'cloud', 'myself', 'scientist', 'popular', 'project', 'creator', 'resistant', 'francisco', 'combine', 'potential', 'bit', 'reaction', 'aws', 'language', 'become', 'amplab', 'help', 'pig', 'reason', 'storm', 'evaluate', 'primary', 'ability', \"'s\", 'seven', 'start', 'stand', 'later', 'matei', 'clear', 'why', 'processing', 'community', 'immediate', 'tool', 'infrastructure', '0.4', 'feature', 'invite', 'audience', 'meetup', 'san', 'remember', 'zaharia'}\n","a1.json {'learning', 'time', 'transform', 'change', 'first', 'year', 'intelligence', 'annual', 'care', 'exclusively', '35', 'continue', 'breakthrough', 'software', 'couple', 'obsess', 'area', 'third', 'figure', 'still', 'fanciest', 'hear', 'v1.0', 'lot', 'grow', 'series', 'good', 'datum', 'deep', 'recruit', 'invest', 'scratch', 'problem', \"'ve\", 'mix', 'method', 'goodness', 'word', 'overwhelmingly', 'been', 'solve', 'different', 'ago', 'academic', 'development', 'last', 'represent', 'public', 'right', 'healthy', 'inbound', 'biggest', 'big', 'whether', 'founder', 'those', 'technology', 'has', 'case', 'landscape', 'meaningful', 'focus', 'future', 'machine', 'private', 'comprehensive', 'work', 'apply', 'people', 'are', 'phrase', 'almost', 'fund', 'surface', 'hype', 'favor', 'business', 'company', 'thoughtfully', 'security', \"'s\", 'inquiry', 'exist', 'publish', 'wonderful', 'investor', 'did', 'domain', 'activity', 'equally', 'even', 'futile'}\n"]}],"source":["import pynlp\n","\n","files = [\"a4.json\", \"a3.json\", \"a2.json\", \"a1.json\"]\n","\n","stopwords = pynlp.load_stopwords(\"stop.txt\")\n","files_set = {}\n","files_mh = {}\n","\n","for json_file in files:\n","    keywords = set([])\n","\n","    for lex in pynlp.lex_iter(json_file):\n","        if (lex.pos != \".\") and (lex.root not in stopwords):\n","            keywords.add(lex.root)\n","\n","    files_set[json_file] = keywords\n","    files_mh[json_file] = mh_digest(keywords)\n","\n","    print(json_file, keywords)"]},{"cell_type":"markdown","id":"deadly-adelaide","metadata":{"editable":true,"id":"deadly-adelaide"},"source":["Let's compare the HTML documents, using a pairwise <font color='blue'>MinHash</font>:"]},{"cell_type":"code","execution_count":null,"id":"thermal-primary","metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"id":"thermal-primary","outputId":"2de1c37b-b4f3-4fee-d41e-681ca5eae13a"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.0762\ta4.json\ta2.json\n","0.0723\ta4.json\ta1.json\n","0.0684\ta4.json\ta3.json\n","0.0664\ta3.json\ta2.json\n","0.0566\ta3.json\ta1.json\n","0.0410\ta2.json\ta1.json\n"]}],"source":["import itertools\n","\n","sim = []\n","\n","for i1, i2 in itertools.combinations(range(len(files)), 2):\n","    j = files_mh[files[i1]].jaccard(files_mh[files[i2]])\n","    sim.append((j, files[i1], files[i2],))\n","\n","for jaccard, file1, file2 in sorted(sim, key=lambda x: x[0], reverse=True):\n","    print(\"%0.4f\\t%s\\t%s\" % (jaccard, file1, file2))"]},{"cell_type":"markdown","id":"finished-newfoundland","metadata":{"editable":true,"id":"finished-newfoundland"},"source":["Note the top-ranked (\"most similar\") pair, where both `html/article2.html` and `html/article4.html` are about learning. Take a look at their overlapping keywords:"]},{"cell_type":"code","execution_count":null,"id":"adequate-stage","metadata":{"id":"adequate-stage","outputId":"ec0df02b-be62-4b30-87eb-475c57630381"},"outputs":[{"data":{"text/plain":["{\"'s\",\n"," 'attendance',\n"," 'book',\n"," 'early',\n"," 'feedback',\n"," 'has',\n"," 'interact',\n"," 'learn',\n"," 'learning',\n"," \"o'reilly\",\n"," 'provide',\n"," 'start',\n"," 'student',\n"," 'was',\n"," 'were'}"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["files_set[\"a4.json\"] & files_set[\"a2.json\"]"]},{"cell_type":"code","execution_count":null,"id":"swedish-sheriff","metadata":{"id":"swedish-sheriff","outputId":"044bd2c5-8c80-4e3e-8add-3b0e59684fb1"},"outputs":[{"data":{"text/plain":["{\"'s\",\n"," \"'ve\",\n"," 'apply',\n"," 'been',\n"," 'company',\n"," 'has',\n"," 'learning',\n"," 'people',\n"," 'publish',\n"," 'whether',\n"," 'year'}"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["# 2nd most similar\n","files_set[\"a4.json\"] & files_set[\"a1.json\"]"]},{"cell_type":"code","execution_count":null,"id":"whole-certificate","metadata":{"id":"whole-certificate"},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"solar-anxiety","metadata":{"editable":true,"id":"solar-anxiety"},"source":["### Example: Word2Vect\n","\n","The data file `terms.tsv` has 10K elements, which is a subset from a **much** larger file.\n","This represents the keyphrases from 843 unique documents.\n","Realistically, you want many more documents in a *Word2Vec* model before the results begin to make a lot of sense.\n","\n","Even so, this is enough to show how to call the functions from [gensim](https://radimrehurek.com/gensim/models/word2vec.html)."]},{"cell_type":"code","execution_count":null,"id":"tough-glenn","metadata":{"editable":true,"id":"tough-glenn"},"outputs":[],"source":["import csv\n","import gensim\n","import logging\n","import sys\n","\n","model_file = \"model.dat\"\n","term_path = \"terms.tsv\""]},{"cell_type":"markdown","id":"specified-myrtle","metadata":{"editable":true,"id":"specified-myrtle"},"source":["Load the parsed keyphrases into a list called `sentences`, where each \"sentence\" is the list of keyphrases from one document."]},{"cell_type":"code","execution_count":null,"id":"competent-appearance","metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"id":"competent-appearance","outputId":"d8eb48ef-c229-4f10-efda-81052b6dbbe2"},"outputs":[{"name":"stdout","output_type":"stream","text":["843\n"]}],"source":["sentences = []\n","sent = []\n","last_doc = None\n","\n","with open(term_path) as f:\n","    for term, doc, rank in csv.reader(f, delimiter=\"\\t\"):\n","        rank = float(rank)\n","\n","        if doc != last_doc:\n","            if last_doc:\n","                sentences.append(sent)\n","                sent = []\n","\n","            last_doc = doc\n","\n","        sent.append(term)\n","\n","    # handle the dangling last element\n","    sentences.append(sent)\n","\n","print(len(sentences))"]},{"cell_type":"markdown","id":"reported-parish","metadata":{"editable":true,"id":"reported-parish"},"source":["Set up logging (which is required by `gensim`) then train `word2vec` on each \"sentence\". Then save the model to the `model.dat` file."]},{"cell_type":"code","execution_count":null,"id":"subsequent-cowboy","metadata":{"editable":true,"id":"subsequent-cowboy"},"outputs":[],"source":["FORMAT = \"%(asctime)s : %(levelname)s : %(message)\"\n","logging.basicConfig(format=FORMAT, level=logging.ERROR)\n","\n","model_new = gensim.models.Word2Vec(sentences, min_count=1, size=200) # min_count=1\n","model_new.save(model_file)"]},{"cell_type":"code","execution_count":null,"id":"applicable-sugar","metadata":{"id":"applicable-sugar","outputId":"5aa7ea18-3c07-4fa5-8d6c-4ec19a82cf5f"},"outputs":[{"data":{"text/plain":["\u001b[0;31mSignature:\u001b[0m \u001b[0mmodel_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mDocstring:\u001b[0m\n","Save the model.\n","This saved model can be loaded again using :func:`~gensim.models.word2vec.Word2Vec.load`, which supports\n","online training and getting vectors for vocabulary words.\n","\n","Parameters\n","----------\n","fname : str\n","    Path to the file.\n","\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/AML_TF/lib/python3.7/site-packages/gensim/models/word2vec.py\n","\u001b[0;31mType:\u001b[0m      method\n"]},"metadata":{},"output_type":"display_data"}],"source":["model_new.save? # model saved in package 'gensim' directory"]},{"cell_type":"code","execution_count":null,"id":"mathematical-copying","metadata":{"id":"mathematical-copying"},"outputs":[],"source":["# gensim.models.Word2Vec?"]},{"cell_type":"markdown","id":"civilian-disco","metadata":{"editable":true,"id":"civilian-disco"},"source":["If you need to load a trained model, use:\n","`model = gensim.models.Word2Vec.load(model_file)`"]},{"cell_type":"code","execution_count":null,"id":"renewable-smile","metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"id":"renewable-smile","outputId":"8546debb-10e8-48c5-ede7-3c1ffa398884"},"outputs":[{"data":{"text/plain":["['-rw-r--r--  1 iordan  staff   3.0M Mar 17 15:48 model.dat',\n"," '-rwxr-xr-x@ 1 iordan  staff   371K Nov 15  2017 terms.tsv']"]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["%sx ls -lth model.dat terms.tsv # model_new.dat"]},{"cell_type":"markdown","id":"direct-treasure","metadata":{"editable":true,"id":"direct-treasure"},"source":["Here's a helper method, which queries the resulting model for \"neighbor\" keyphrases:"]},{"cell_type":"code","execution_count":null,"id":"specific-sharing","metadata":{"editable":true,"id":"specific-sharing"},"outputs":[],"source":["def get_synset (model, query, topn=2):\n","    try:\n","        return sorted(model.most_similar(positive=[query], topn=topn), key=lambda x: x[1], reverse=True)\n","    except KeyError:\n","        return []"]},{"cell_type":"markdown","id":"driving-conflict","metadata":{"editable":true,"id":"driving-conflict"},"source":["Now we can query the model interactively through a mini REPL:"]},{"cell_type":"code","execution_count":null,"id":"equivalent-wound","metadata":{"id":"equivalent-wound"},"outputs":[],"source":["# try: market, hotel\n","\n","# to stop: EXIT!"]},{"cell_type":"code","execution_count":null,"id":"ideal-august","metadata":{"editable":true,"jupyter":{"outputs_hidden":false},"id":"ideal-august","outputId":"023ec7e0-9f7f-4658-86ee-b2bccb3a9b9b"},"outputs":[{"name":"stdin","output_type":"stream","text":["\n","query?  market\n"]},{"name":"stderr","output_type":"stream","text":["/Users/iordan/anaconda3/envs/AML_TF/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"]},{"name":"stdout","output_type":"stream","text":["most similar to market : [('time', 0.9984865188598633), ('has', 0.9984476566314697), ('go', 0.998352587223053), ('home', 0.9982441663742065), ('business', 0.998213529586792), ('make', 0.9981980323791504), ('world', 0.9981462955474854), ('information', 0.9981448650360107), ('investment', 0.9981405735015869), ('data', 0.9981072545051575)]\n"]},{"name":"stdin","output_type":"stream","text":["\n","query?  EXIT!\n"]}],"source":["NUM_RESULTS = 10\n","\n","while True:\n","    try:\n","        query = input(\"\\nquery? \")\n","        if query=='EXIT!':\n","            break\n","        synset = get_synset(model_new, query, topn=NUM_RESULTS)\n","        print(\"most similar to\", query, \":\", synset)\n","    except KeyError:\n","        print(\"not found\")"]},{"cell_type":"code","execution_count":null,"id":"specified-pricing","metadata":{"id":"specified-pricing"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"AML7_NLP.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":5}